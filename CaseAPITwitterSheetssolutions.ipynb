{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supreme-diamond",
   "metadata": {},
   "source": [
    "# Analyse d'e-r√©putation Twitter ‚Äî Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-matrix",
   "metadata": {},
   "source": [
    "Ce projet a pour objectif de r√©capituler (quasi) toutes les comp√©tences vues dans cette formation pour analyser une e-r√©putation Twitter, en 3 temps :\n",
    "* Recherche des tweets avec l'API Twitter\n",
    "* Nettoyage (pre-processing) et analyse des tweets\n",
    "* Envoi dans un spreadsheet avec l'API Google Sheets.\n",
    "___\n",
    "\n",
    "Vous √™tes data analyst chez Olist, et vous aimeriez savoir comment vos concurrents sont per√ßus par le public. Vous codez donc avec Python un programme qui utilise l'API Twitter pour **rechercher les tweets qui √©voquent vos concurrents** (par exemple Amazon). Le programme ensuite les **nettoie**, les **analyse** (mots les plus courants, √©motions les plus pr√©sentes, r√©partition g√©ographique, sous forme de graphes et de statistiques) et enfin envoie les r√©sultats de ces analyses dans un **spreadsheet**. \n",
    "\n",
    "Ce genre d'analyse peut ensuite √™tre **r√©p√©t√©**, par exemple chaque semaine, afin de surveiller l'√©tat du march√© et **l'√©volution de la perception** que les clients ont de vos concurrents (ou de votre entreprise elle-m√™me !). Le projet peut s'adapter √† de nombreux usages !\n",
    "\n",
    "Dans ce notebook, vous serez guid√© dans **toutes les √©tapes** de la cr√©ation de cet outil. Nous utiliserons les API de Twitter et de Google Sheets, vous aurez donc besoin de **configurer un compte Google Cloud et un compte Twitter Developer** en suivant les instructions disponibles sur la plateforme Databird (**2 fichiers PDF**). Allons-y !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informational-diving",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:54:14.961741Z",
     "start_time": "2021-09-17T16:53:14.311413Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting requests-oauthlib<2,>=1.2.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting requests<3,>=2.27.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Installing collected packages: requests, oauthlib, requests-oauthlib, tweepy\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "Successfully installed oauthlib-3.2.2 requests-2.31.0 requests-oauthlib-1.3.1 tweepy-4.14.0\n",
      "Collecting gspread\n",
      "  Downloading gspread-5.11.2-py3-none-any.whl (46 kB)\n",
      "Collecting google-auth>=1.12.0\n",
      "  Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from google-auth>=1.12.0->gspread) (5.0.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (1.26.7)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, gspread\n",
      "Successfully installed google-auth-2.23.2 google-auth-oauthlib-1.1.0 gspread-5.11.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 rsa-4.9\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (0.5.0)\n",
      "Collecting httplib2>=0.9.1\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from httplib2>=0.9.1->oauth2client) (3.0.4)\n",
      "Installing collected packages: httplib2, oauth2client\n",
      "Successfully installed httplib2-0.22.0 oauth2client-4.1.3\n",
      "Requirement already satisfied: nltk in c:\\users\\azade\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\azade\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.8.0\n",
      "Requirement already satisfied: wordcloud in c:\\users\\azade\\anaconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\azade\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\azade\\anaconda3\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\azade\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Collecting geopy\n",
      "  Downloading geopy-2.4.0-py3-none-any.whl (125 kB)\n",
      "Collecting geographiclib<3,>=1.52\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.0\n",
      "Collecting folium\n",
      "  Downloading folium-0.14.0-py2.py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\azade\\anaconda3\\lib\\site-packages (from folium) (2.31.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from folium) (2.11.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\azade\\anaconda3\\lib\\site-packages (from folium) (1.20.3)\n",
      "Collecting branca>=0.6.0\n",
      "  Downloading branca-0.6.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (1.26.7)\n",
      "Installing collected packages: branca, folium\n",
      "Successfully installed branca-0.6.0 folium-0.14.0\n"
     ]
    }
   ],
   "source": [
    "# ex√©cuter cette cellule si tous les packages ne sont pas install√©s\n",
    "!pip install tweepy\n",
    "!pip install gspread \n",
    "!pip install oauth2client\n",
    "!pip install nltk\n",
    "!pip install emoji\n",
    "!pip install wordcloud\n",
    "!pip install geopy\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-desire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:55:01.151147Z",
     "start_time": "2021-09-17T16:54:43.823969Z"
    }
   },
   "outputs": [],
   "source": [
    "# packages g√©n√©ralistes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# packages d'analyse de texte\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import emoji\n",
    "\n",
    "# packages graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, HeatMap\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# API wrappers Twitter et Google Sheets\n",
    "import tweepy\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-galaxy",
   "metadata": {},
   "source": [
    "## API Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-cotton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T23:30:25.888239Z",
     "start_time": "2021-04-18T23:30:25.871579Z"
    }
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-theory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:54:31.746628Z",
     "start_time": "2021-09-17T16:54:31.703225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remplacez par vos propres cl√©s d'API Twitter\n",
    "API_KEY = 'diVvpJ68bD2T9Z7oSWcqRFSeh'\n",
    "API_SECRET_KEY = 'A8WSmaeMahMkUDTI1oG8Q0OSaBCOswxCMCCK4JfkmyzsVBcE0R'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAJXNOgEAAAAAtUAkIFZgT38EsEGEMA%2BWXWZU3n4%3Daa9H6Jd9k7ddlaC92144L55mOqP9Nl4JBHhYLZyufTC50RNbxE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-beauty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:55:01.878474Z",
     "start_time": "2021-09-17T16:55:01.155666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authentification et cr√©ation du client d'API\n",
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET_KEY)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-spell",
   "metadata": {},
   "source": [
    "### Recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-craft",
   "metadata": {},
   "source": [
    "1. **Recherchez les tweets √† propos d'Amazon** (ou de **Tesla** ‚Äî souvent plus amusants, d'exp√©rience). Vous pouvez ajouter d'autres termes de recherche dans votre requ√™te et ajuster les param√®tres comme vous le souhaitez. Pour l‚Äôinstant, limitez votre recherche √† **100 tweets**.\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* Utilisez la fonction `tweepy.Cursor` vue dans le live-coding.\n",
    "* Lorsque vous recherchez un mot, l‚ÄôAPI Twitter le recherche dans toutes donn√©es li√©es √† un tweet (texte du tweet, hashtags, nom d‚Äôutilisateur, etc.).\n",
    "* Je vous conseille de filtrer les retweets avec `-filter:retweets` pour obtenir des tweets plus pertinents.\n",
    "* Le r√©sultat de la fonction `tweepy.Cursor` est un \"g√©n√©rateur\". Convertissez ce r√©sultat en liste √† l‚Äôaide de la fonction `list()`.\n",
    "* Une fois convertis en liste, vous pouvez it√©rer sur les tweets pour en extraire des informations. Par exemple, pour extraire le texte des tweets, utilisez l‚Äôattribut `.full_text` sur les tweets dans votre boucle (voir live-coding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-syria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:56:24.555012Z",
     "start_time": "2021-09-17T16:56:24.512851Z"
    }
   },
   "outputs": [],
   "source": [
    "query =\"tesla -filter:retweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-bronze",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:57:33.937595Z",
     "start_time": "2021-09-17T16:57:29.071355Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recherche des tweets\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "                       q=query,\n",
    "                       lang=\"en\",\n",
    "                       tweet_mode=\"extended\",\n",
    "                       #since=\"2021-06-18\", # put here the last week\n",
    "                       #until=\"2021-06-25\",\n",
    "                      ).items(50)\n",
    "tweets = list(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-little",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:58:16.746455Z",
     "start_time": "2021-09-17T16:58:16.414730Z"
    }
   },
   "outputs": [],
   "source": [
    "# V√©rification : extraction du texte des tweets\n",
    "\n",
    "tweets_text = []\n",
    "for tweet in tqdm(tweets):\n",
    "    tweets_text.append(tweet.full_text)\n",
    "tweets_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-engine",
   "metadata": {},
   "source": [
    "2. Int√©ressons-nous seulement √† **1 tweet** pour l'instant (un tweet complet, dans le r√©sultat de `tweepy.Cursor`, pas seulement le texte d'un tweet). **Extraire les attributs les plus int√©ressants** de ce tweet √† partir de la r√©ponse au format **json**, et stocker ces informations dans une liste.\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* La r√©ponse json fournie pour chaque tweet dans les r√©sultats contient beaucoup plus d‚Äôinformations que le simple texte du tweet. Commencez par extraire la r√©ponse json compl√®te d'un seul tweet avec `._json`, pour comprendre comment les informations sont organis√©es.\n",
    "* Puis utilisez la syntaxe `.attribut` (`.full_text`, `.user.screen_name`, etc.) pour extraire le texte du tweet, sa date de cr√©ation, ses hashtags, la localisation de l'utilisateur, le nombre de followers de l'utilisateur, le nombre de statuts publi√©s par l'utilisateur, le nombre de retweets et le nombre de favoris du tweet.\n",
    "* Stockez toutes ces informations dans une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-savings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:59:19.962609Z",
     "start_time": "2021-09-17T16:59:19.843976Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# R√©ponse json compl√®te pour 1 tweet\n",
    "\n",
    "test = list(tweets)[10]\n",
    "json_test = test._json\n",
    "json_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-sussex",
   "metadata": {},
   "source": [
    "On souhaite extraire les attributs : `full_text`, `created_at`, `entities.hashtags`, `user.location`, `user.followers_count`, `user.statuses_count`, `retweet_count`, `favorite_count` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-think",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:07:50.682472Z",
     "start_time": "2021-09-17T17:07:50.646927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extraction des informations pertinentes\n",
    "\n",
    "info = [test.full_text, \n",
    "        test.created_at,\n",
    "        #test.entities['hashtags'], # ne fonctionne pas en l'√©tat ! \n",
    "        [h['text'] for h in test.entities['hashtags']], # solution d√©taill√©e dessous\n",
    "        test.user.location, \n",
    "        test.user.followers_count, \n",
    "        test.user.statuses_count, \n",
    "        test.retweet_count, \n",
    "        test.favorite_count]\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-performer",
   "metadata": {},
   "source": [
    "Ci-dessous, les quelques tests effectu√©s afin d'√©crire la liste compr√©hension permettant d'extraire les hashtags :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-respect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:01:16.566008Z",
     "start_time": "2021-09-17T17:01:16.526378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experience = [{'text': 'dogecoin', 'indices': [39, 48]},\n",
    "  {'text': 'doge', 'indices': [49, 54]}]\n",
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-tribune",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:02:26.267982Z",
     "start_time": "2021-09-17T17:02:26.223657Z"
    }
   },
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "for x in experience:\n",
    "    hashtag = x['text']\n",
    "    hashtags.append(hashtag)\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-entertainment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:02:30.871999Z",
     "start_time": "2021-09-17T17:02:30.836773Z"
    }
   },
   "outputs": [],
   "source": [
    "[h['text'] for h in experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-knitting",
   "metadata": {},
   "source": [
    "3. R√©pliquez le m√™me processus sur **tous les tweets** issus de la recherche.\n",
    "\n",
    "_Indices : Vous aurez probablement besoin d'une boucle. Vous pouvez utiliser **`tqdm`** pour afficher une barre de progression. L'objectif est d'obtenir une **liste de listes** (chaque sous-liste correspondant aux informations d'1 tweet)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-inflation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:03:57.031512Z",
     "start_time": "2021-09-17T17:03:56.792561Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extraction des informations pour tous les tweets\n",
    "\n",
    "tweets_infos = []\n",
    "for tweet in tqdm(tweets):\n",
    "    tweet_info = [tweet.full_text, \n",
    "                    tweet.created_at, \n",
    "                    [h['text'] for h in tweet.entities['hashtags']], \n",
    "                    tweet.user.location, \n",
    "                    tweet.user.followers_count, \n",
    "                    tweet.user.statuses_count, \n",
    "                    tweet.retweet_count, \n",
    "                    tweet.favorite_count] \n",
    "    tweets_infos.append(tweet_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-quebec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:04:01.423178Z",
     "start_time": "2021-09-17T17:04:01.363631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-heavy",
   "metadata": {},
   "source": [
    "4. Convertir les r√©sultats en **dataframe** (une ligne par tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-blink",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:04:57.849464Z",
     "start_time": "2021-09-17T17:04:57.716305Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets_infos)\n",
    "df.columns = ['full_text', 'created_at', 'hashtags', 'location',\n",
    "              'followers', 'statuses', 'retweets', 'favorites']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-scale",
   "metadata": {},
   "source": [
    "### Nettoyage (pre-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-enclosure",
   "metadata": {},
   "source": [
    "On souhaite ici pr√©parer le **texte** des tweets pour l'analyse. Une bonne pratique consiste √† **tester toutes les √©tapes de pre-processing sur 1 seul tweet**, avant de les **appliquer √† tous les tweets dans une boucle**. Et c'est ce que nous allons faire !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-judges",
   "metadata": {},
   "source": [
    "0. Isolez le texte d'un seul tweet pour pouvoir tester les √©tapes de pre-processing lors des questions suivantes.\n",
    "\n",
    "_Conseil : C'est encore mieux si votre tweet contient des @, # et des liens hypertexte, que nous nous efforcerons de supprimer. Si vous ne trouvez pas de tweet qui convient, √©crivez simplement un faux tweet de test !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-ordering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:11:04.133976Z",
     "start_time": "2021-09-17T17:11:04.038487Z"
    }
   },
   "outputs": [],
   "source": [
    "test = 'Come spend Christmas üëë with Amazon! https://t.co/r8oG #Christmas @amazon'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-plastic",
   "metadata": {},
   "source": [
    "1. Supprimer les liens, identifications (caract√®re @ et nom d'utilisateur, peu utiles √† l'analyse) et tags (caract√®re # seulement), gr√¢ce √† la fonction `clean_tweet` ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-determination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:11:38.642005Z",
     "start_time": "2021-09-17T17:11:38.569260Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_tweet(txt):\n",
    "    return re.sub(\"(http\\S+)|(@\\S+)\", \"\", txt).replace('#', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-inclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:14:18.602288Z",
     "start_time": "2021-09-17T17:14:18.569047Z"
    }
   },
   "outputs": [],
   "source": [
    "test = clean_tweet(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-tuner",
   "metadata": {},
   "source": [
    "2. Mettre le texte en minuscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-crown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:14:30.045914Z",
     "start_time": "2021-09-17T17:14:30.018610Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test.lower()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-clarity",
   "metadata": {},
   "source": [
    "3. Supprimer tous les signes de ponctuation et les remplacer par des espaces.\n",
    "\n",
    "_Conseils : Vous aurez sans doute besoin d'une \"regex\" (expression r√©guli√®re), comme dans la fonction `clean_tweet`. Attention √† ne pas supprimer les emojis. Vous pouvez utiliser la regex : `re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', test_tweet)`. Cette regex signifie : \"remplacer tous les caract√®res qui ne sont pas (`^`) des lettres (`\\w`), des espaces (`\\s`) ou des caract√®res unicode (`\\U`, sur tous les codes possibles) par un espace\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-atlantic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:15:54.326009Z",
     "start_time": "2021-09-17T17:15:54.283458Z"
    }
   },
   "outputs": [],
   "source": [
    "test = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-fetish",
   "metadata": {},
   "source": [
    "4. Scinder le texte en mots individuels et supprimer les espaces avant et apr√®s chaque mot (\"leading and trailing white spaces\") si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-earth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:16:42.301944Z",
     "start_time": "2021-09-17T17:16:42.260114Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test.split()\n",
    "test  # .strip() √† appliquer sur tous les mots si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-victim",
   "metadata": {},
   "source": [
    "5. Supprimer les mots de la requ√™te (\"collection words\").\n",
    "\n",
    "_Indice : Vous pouvez utiliser une list comprehension incluant une condition._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-glory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:18:13.788825Z",
     "start_time": "2021-09-17T17:18:13.772846Z"
    }
   },
   "outputs": [],
   "source": [
    "collection_words = ['amazon']\n",
    "test = [ word for word in test if not word in collection_words ]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-stephen",
   "metadata": {},
   "source": [
    "6. Supprimer les \"stopwords\", c'est-√†-dire les mots courants qui ne portent pas de sens (\"the\", \"a\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-opinion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:18:47.161285Z",
     "start_time": "2021-09-17T17:18:47.115488Z"
    }
   },
   "outputs": [],
   "source": [
    "# T√©l√©charger la liste de stopwords du package nltk \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(list(stop_words)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-tonight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:19:03.296435Z",
     "start_time": "2021-09-17T17:19:03.287990Z"
    }
   },
   "outputs": [],
   "source": [
    "test = [ word for word in test if not word in stop_words ]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-summit",
   "metadata": {},
   "source": [
    "7. Appliquer les √©tapes pr√©c√©dentes √† tous les tweets. L'objectif est d'obtenir un long corpus de texte, i.e. une longue liste contenant tous les mots indicviduels issus des tweets nettoy√©s.\n",
    "\n",
    "_Conseils :_\n",
    "* Commencez par cr√©er une liste contentant le texte de tous vos tweets.\n",
    "* Utilisez ensuite une boucle pour appliquer les √©tapes de nettoyage √† chaque tweet.\n",
    "* Souvenez-vous de la diff√©rence entre `.append` et `.extend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-break",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:19:51.287217Z",
     "start_time": "2021-09-17T17:19:51.276082Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A partir du r√©sultat de la recherche, conserver seulement le texte des tweets\n",
    "tweets_text = [t[0] for t in tweets_infos]\n",
    "tweets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-stomach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:21:39.997453Z",
     "start_time": "2021-09-17T17:21:39.883454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construire le corpus de mots en appliquant les √©tapes test√©es plus haut\n",
    "corpus = []\n",
    "for tweet in tweets_text:\n",
    "    clean = clean_tweet(tweet).lower()\n",
    "    clean = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', clean)\n",
    "    clean = clean.split()\n",
    "    collection_words = ['tesla']\n",
    "    clean = [word for word in clean if not word in collection_words]\n",
    "    clean = [word for word in clean if not word in stop_words]\n",
    "    corpus.extend(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-business",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:21:41.752114Z",
     "start_time": "2021-09-17T17:21:41.698493Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-intro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:22:24.128258Z",
     "start_time": "2021-09-17T17:22:24.102164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Version fonctionnelle\n",
    "\n",
    "def clean_full(tweet: str):\n",
    "    clean = clean_tweet(tweet).lower()\n",
    "    clean = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', clean)\n",
    "    clean = clean.split()\n",
    "    collection_words = ['amazon']\n",
    "    clean = [word for word in clean if not word in collection_words]\n",
    "    clean = [word for word in clean if not word in stop_words]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-grace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:22:38.351516Z",
     "start_time": "2021-09-17T17:22:38.339258Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for tweet in tweets_text:\n",
    "    result = clean_full(tweet)\n",
    "    corpus.extend(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-liechtenstein",
   "metadata": {},
   "source": [
    "8. Excellent, nous avons d√©sormais un corpus de mots pr√™t √† l'analyse ! Pour cette derni√®re question de la section, cr√©er un nouvelle colonne dans le dataframe de la section pr√©c√©dente. Mettre dans cette colonne, le text des tweets, en leur appliquant uniquement les √©tapes 1 and 2 du cleaning (supprimer les @, # et les links, et mettre en minuscules). Nommer cette colonne `clean_text`. Nous l'utilserons aussi pour l'analyse.\n",
    "\n",
    "_Conseil : Vous pouvez utiliser `apply`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-shadow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:23:18.632061Z",
     "start_time": "2021-09-17T17:23:18.570955Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['full_text'].apply(lambda x: clean_tweet(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-upset",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:23:21.726462Z",
     "start_time": "2021-09-17T17:23:21.594880Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-barrier",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-problem",
   "metadata": {},
   "source": [
    "Dans cette section, nous m√®nerons plusieurs analyses fond√©es sur le corpus de mots et sur le dataframe contenant les tweets complets nettoy√©s, ainsi que les autres informations issues de notre recherche."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-cannon",
   "metadata": {},
   "source": [
    "#### Mots les plus fr√©quents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-spine",
   "metadata": {},
   "source": [
    "0. Compter le nombre de mots uniques dans le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-witness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:12:08.757199Z",
     "start_time": "2021-06-28T15:12:08.751562Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(set(corpus))} words in the combination of all searched tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-pharmacology",
   "metadata": {},
   "source": [
    "1. Compter le nombre d'occurences de chaque mot. Quels sont les 10 mots les plus fr√©quents ?\n",
    "\n",
    "_Indices : Souvenez-vous de `Counter()` du package `collections`. Essayez-aussi d'appliquer `.most_common(10)` au r√©sultat de votre compteur._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-coral",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:16:05.681320Z",
     "start_time": "2021-06-28T15:16:05.649398Z"
    }
   },
   "outputs": [],
   "source": [
    "top10_words = Counter(corpus).most_common(10)\n",
    "top10_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-institution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:16:28.760420Z",
     "start_time": "2021-06-28T15:16:28.743046Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(corpus).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-stevens",
   "metadata": {},
   "source": [
    "2. Afficher la fr√©quence des n premiers mots du corpus (n doit √™tre modifiable) dans un diagramme en b√¢tons (bar plot).\n",
    "\n",
    "_Indice : Vous pouvez cr√©er le bar plot manuellement, ou bien utiliser la fonction du notebook solutions (plus rapide !)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-legend",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:20:21.889725Z",
     "start_time": "2021-06-28T15:20:21.877548Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_frequencies(words, top=15):\n",
    "    \n",
    "    # Create dataframe for plotting\n",
    "    frequency_df = pd.DataFrame(Counter(words).most_common(top),\n",
    "                             columns=['words', 'count'])\n",
    "    \n",
    "    # Plot\n",
    "    frequency_df.sort_values(by='count').plot.barh(x='words',\n",
    "                                                   y='count',\n",
    "                                                   color=\"red\")\n",
    "    plt.title(\"Most Common Words in Tweets\", fontweight='semibold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-survivor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:20:23.442983Z",
     "start_time": "2021-06-28T15:20:22.736545Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_frequencies(corpus, top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-edmonton",
   "metadata": {},
   "source": [
    "3. Utiliser la fonction ci-dessous pour afficher un nuage de mots (wordcloud) des mots les plus communs dans le corpus.\n",
    "\n",
    "_Note : Vous pouvez inclure un argument `mask` dans la fonction, avec le chemin d'une image qui donnera forme au nuage de mots. Il peut s'agir de (quasi) n'importe quelle image en noir et blanc ‚Äî en particulier, vous pouvez utiliser le fichier `glasses.jpg` disponible sur la plateforme._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-batch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:24:17.227971Z",
     "start_time": "2021-06-28T15:24:17.215993Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordcloud(corpus, title=None, mask=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Draws a word cloud with an optional mask shape, using the wordcloud package.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    corpus: list\n",
    "        List of all individual words to be used for the word cloud.\n",
    "    \n",
    "    title: string, optional, default: None\n",
    "        The title of your word cloud.\n",
    "\n",
    "    mask: string, optional, default: None\n",
    "        The path to the image to be used as mask. It must be binary (black & white).\n",
    "        \n",
    "    figsize: tuple, optional, default: (10,10)\n",
    "        The size of the chart area.\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocess data\n",
    "    corpus = str(corpus).replace(\"'\", \"\")\n",
    "    \n",
    "    # load mask if provided\n",
    "    if mask != None:\n",
    "        mask_ = Image.open(mask)\n",
    "        fn = lambda x : 255 if x >= 200 else 0\n",
    "        mask_ = mask_.convert('L').point(fn, mode='1').convert('RGB')\n",
    "        mask_ = np.array(mask_)\n",
    "    else: \n",
    "        mask_ = None\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        mask = mask_,\n",
    "        max_font_size = 45,\n",
    "        min_font_size = 5,\n",
    "        contour_width = 0.1,\n",
    "        contour_color = 'silver',\n",
    "        repeat = True,\n",
    "        stopwords=STOPWORDS,\n",
    "        random_state = 1).generate(str(corpus))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=figsize)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=18, y=0.75)\n",
    "    plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-stuart",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:24:18.746873Z",
     "start_time": "2021-06-28T15:24:17.900185Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud(corpus, title=\"Most Common Words in Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-mainland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:24:52.553134Z",
     "start_time": "2021-06-28T15:24:50.406007Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud(corpus, title=\"Most Common Words in Tweets\", mask='glasses.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-buffer",
   "metadata": {},
   "source": [
    "#### Hashtags et emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-entity",
   "metadata": {},
   "source": [
    "1. Quels sont les hashtags les plus fr√©quents dans nos tweets ? (top 10)\n",
    "\n",
    "_Indices : Vous aurez sans doute besoin \"d'aplatir\" (\"flatten\") une liste de listes, c'est-√†-dire transformer une liste de listes en une seule longue liste. Pour cela, vous pouvez utiliser le package `itertools` : `simple_list = list(itertools.chain(*list_of_lists))`. Souvenez-vous aussi de mettre tous les hashtags en minuscules._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-accreditation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:45:51.375439Z",
     "start_time": "2021-06-28T15:45:51.324424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtags = list(df['hashtags'].values)\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-tablet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:48:26.358366Z",
     "start_time": "2021-06-28T15:48:26.348910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtags = list(itertools.chain(*hashtags))\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-lawyer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:49:38.411659Z",
     "start_time": "2021-06-28T15:49:38.402059Z"
    }
   },
   "outputs": [],
   "source": [
    "# mettre en minuscules pour √©viter les doublons\n",
    "hashtags = [ h.lower() for h in hashtags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-estonia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T18:23:02.540771Z",
     "start_time": "2021-04-21T18:23:02.532349Z"
    }
   },
   "outputs": [],
   "source": [
    "top10_hashtags = Counter(hashtags).most_common(10)\n",
    "top10_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-sense",
   "metadata": {},
   "source": [
    "2. Quels sont les emojis les plus fr√©quents ? (top 10)\n",
    "\n",
    "_Indice : La premi√®re √©tape pour cette question est de cr√©er un corpus d'emojis en conservant seulement les emojis dans le corpus de mots complet. Vous pouvez pour cela utiliser la fonction `char_is_emoji` ci-dessous._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-display",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:38.523669Z",
     "start_time": "2021-06-28T15:51:38.518321Z"
    }
   },
   "outputs": [],
   "source": [
    "def char_is_emoji(character):\n",
    "    return character in emoji.UNICODE_EMOJI['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-account",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:39.125056Z",
     "start_time": "2021-06-28T15:51:39.110498Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_corpus = []\n",
    "for word in corpus:\n",
    "    for character in word:\n",
    "        if char_is_emoji(character):\n",
    "            emoji_corpus.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-pledge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:39.828987Z",
     "start_time": "2021-06-28T15:51:39.804509Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emoji_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-shipping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:53.502596Z",
     "start_time": "2021-06-28T15:51:53.473177Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top10_emojis = Counter(emoji_corpus).most_common(10)\n",
    "top10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2d7dd",
   "metadata": {},
   "source": [
    "#### Analyse de sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801dfaa",
   "metadata": {},
   "source": [
    "Parlons maintenant de **NLP, \"Natural Language Processing\"** (parfois traduit \"Traitement Automatique du Langage Naturel\", TALN). Le NLP est un champ de recherche du Deep Learning, l'apprentissage profond, qui s'int√©resse √† l'analyse de texte et √† la construction de mod√®les pr√©dictifs li√©s au langage. En particulier, un important domaine d'application du NLP est l'analyse de sentiment, i.e. **pr√©dire le sentiment transmis par une phrase ou un paragraphe**. \n",
    "\n",
    "Nous allons utiliser l'analyse de sentiment sur nos tweets pour essayer de d√©terminer les **sentiments les plus communs quand les internautes √©voquent notre sujet sur Twitter**.\n",
    "\n",
    "La construction d'un mod√®le de NLP est relativement complexe, heureusement le package `nltk` nous fournit plusieurs fonctions de NLP, notamment une **fonction pour faire appel √† un mod√®le d'analyse de sentiment pr√©-entra√Æn√©**. Ex√©cutez la cellule ci-dessous pour initialiser ce mod√®le, puis suivez les questions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e22ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:26:08.378674Z",
     "start_time": "2021-06-28T15:26:08.302895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisation du mod√®le d'analyse de sentiment\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ba0d5",
   "metadata": {},
   "source": [
    "1. Pr√©dire le sentiment d'un seul tweet avec la commande `sia.polarity_scores(test_tweet)`. Le tweet pass√© en argument doit √™tre une phrase compl√®te, i.e. une longue cha√Æne de caract√®res, et non une liste de mots.\n",
    "\n",
    "_Note : Comme vous le verrez, le sentiment est mesur√© en pourcentage, r√©parti entre 'n√©gatif', 'positif' et 'neutre'. La commande renvoie aussi un score composite ('compound'), compris entre -1 et 1. Des explications suppl√©mentaires sur le mod√®le et le calcul des scores sont disponibles sur le GitHub du mod√®le Vader utilis√© par nltk : [link](https://github.com/cjhutto/vaderSentiment#about-the-scoring)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946edcdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:26:36.108576Z",
     "start_time": "2021-06-28T15:26:36.103004Z"
    }
   },
   "outputs": [],
   "source": [
    "test = 'Come spend the Christmas period üëë with Amazon !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731aa28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:30:41.767037Z",
     "start_time": "2021-06-28T15:30:41.760093Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia.polarity_scores(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa6727",
   "metadata": {},
   "source": [
    "2. Calculer le sentiment moyen de tous les tweets.\n",
    "\n",
    "_Indice : Utilisez la colonne `clean_text` de votre dataframe. Le score qui nous int√©resse ici est le score \"compound\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235cde7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:34:37.593694Z",
     "start_time": "2021-06-28T15:34:37.548211Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_tweets = df['clean_text'].values\n",
    "clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea31d4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:36:17.356723Z",
     "start_time": "2021-06-28T15:36:17.315319Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_scores = [ sia.polarity_scores(t)[\"compound\"] for t in clean_tweets ]\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64018355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:36:45.151964Z",
     "start_time": "2021-06-28T15:36:45.118819Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_sentiment = np.mean(sentiment_scores)\n",
    "mean_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7da65f",
   "metadata": {},
   "source": [
    "3. Afficher la r√©partition des sentments sous forme de graphe (histogramme) gr√¢ce √† la fonction ci-dessous.\n",
    "\n",
    "_Note : La majorit√© des tweets seront sans doute class√©s comme \"neutres\", car le mod√®le n'a pas √©t√© entra√Æn√© sur des donn√©es tout √† fait similaires aux n√¥tres, et nos donn√©es ne sont pas pr√©par√©es de mani√®re tout √† fait ad√©quate pour le mod√®le ‚Äî les r√©sultats ne sont donc qu'indicatifs, mais cette premi√®re approche du NLP me semble int√©ressante n√©anmoins !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359d1f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:39:15.115448Z",
     "start_time": "2021-06-28T15:39:15.097820Z"
    }
   },
   "outputs": [],
   "source": [
    "# sentiment analysis plot\n",
    "\n",
    "def plot_sentiments(tweets):\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    scores = []\n",
    "    for t in clean_tweets:\n",
    "        scores.append(sia.polarity_scores(t)['compound'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(scores, bins=3, range=(-1,1), color='red')\n",
    "    ax.set_title(\"Most Common Sentiments in Tweets\")\n",
    "    ax.set_xticks([-0.7, 0, 0.7])\n",
    "    ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de26372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:39:16.076948Z",
     "start_time": "2021-06-28T15:39:15.731760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_sentiments(clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-details",
   "metadata": {},
   "source": [
    "#### Popularit√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-australia",
   "metadata": {},
   "source": [
    "Par popularit√©, on entend \"√† quel point les discussions √† propos de notre sujet sont populaires\", en termes de nombre de retweets et de mises en favoris, et en termes de nombre followers et nombre de statuts des utilisateurs qui postent √† ce sujet. Dans cette section, nous analyserons donc les colonnes `retweets`, `favorites`, `followers` et `statuses` de notre dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-battle",
   "metadata": {},
   "source": [
    "1. Calculer le nombre moyen de followers et de statuts des utilisateurs qui tweetent √† propos de notre sujet (= la popularit√© de notre communaut√©). La m√©diane serait-elle ici plus pertinent que la moyenne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-hebrew",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:54:33.675152Z",
     "start_time": "2021-06-28T15:54:33.667914Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_followers = df['followers'].median()\n",
    "mean_statuses = df['statuses'].median()\n",
    "print(mean_followers)\n",
    "print(mean_statuses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-thanksgiving",
   "metadata": {},
   "source": [
    "2. Calculer la moyenne du nombre de retweets et de favoris de nos tweets (= la popularit√© des tweets sur notre sujet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-equation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:57:06.746900Z",
     "start_time": "2021-06-28T15:57:06.717100Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_retweets = df['retweets'].mean()\n",
    "mean_favorites = df['favorites'].mean()\n",
    "print(mean_retweets)\n",
    "print(mean_favorites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-poster",
   "metadata": {},
   "source": [
    "3. Quel est le tweet le plus populaire de notre requ√™te ?\n",
    "\n",
    "_Indice : On peut consid√©rer que le tweet le plus populaire est celui qui a le plus grand nombre cumul√© de retweets et de favoris._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-apparatus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:57:08.640826Z",
     "start_time": "2021-06-28T15:57:08.518361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sum_favorites_retweets'] = df['favorites'] + df['retweets']\n",
    "top_tweet = df[df['sum_favorites_retweets'] == max(df['sum_favorites_retweets'])]\n",
    "top_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-norfolk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:57:43.133168Z",
     "start_time": "2021-06-28T15:57:43.113831Z"
    }
   },
   "outputs": [],
   "source": [
    "top_tweet = top_tweet['full_text'].values[0]\n",
    "top_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-assembly",
   "metadata": {},
   "source": [
    "#### Localisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4ea49",
   "metadata": {},
   "source": [
    "Nous analyserons dans cette section l'origine des auteurs de nos tweets ‚Äî lorsque cette localisation est connue (d√©pend des informations fournies par l'utilisateur dans son profil)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-watershed",
   "metadata": {},
   "source": [
    "1. Extraire la colonne `locations` de notre dataframe de tweets, et la stocker sous forme de liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-wedding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:58:50.728120Z",
     "start_time": "2021-06-28T15:58:50.698862Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locations = df['location'].values\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-cooper",
   "metadata": {},
   "source": [
    "2. Utiliser la fonction `Nominatim()` du package `geopy.geocoders` pour effectuer un \"reverse geocoding\" (\"g√©ocodage inverse\") des localisations.\n",
    "\n",
    "\n",
    "_Note : Le g√©ocodage inverse consiste √† obtenir la latitude et la longitude d'un lieu √† partir de son nom. Par exemple, la fonction `Nominatim()` peut prendre comme argument \"New York, US\" et renvoyer la latitude et la longitude du centre de New York. En coulisses, cette fonction requ√™te l'API de g√©ocodage inverse \"Nominatim\" qui est g√©r√©e par OpenStreetMap, une version open-source de Google Maps._\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* L'objectif de cette question est de cr√©er un **dictionnaire** contenant 2 cl√©s : \"latitude\" et \"longitude\", et √† l'int√©rieur de chaque cl√© une **liste** de toutes les latitudes (ou longitudes) de nos localisations.\n",
    "* Regardez l'exemple ci-dessous pour voir comment reverse g√©ocoder une seule localisation. Vous pourrez ensuite faire de m√™me sur toutes les localisations dans une boucle, en remplissant au fur et √† mesure votre dictionnaire de r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-cooler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:59:20.544222Z",
     "start_time": "2021-06-28T15:59:20.373038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "geolocator = Nominatim(user_agent=\"my-app\")\n",
    "\n",
    "# Exemple\n",
    "location = 'New York, US'\n",
    "reverse_location = geolocator.geocode(location)\n",
    "lat = reverse_location.latitude\n",
    "lon = reverse_location.longitude\n",
    "print(lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-douglas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:02:23.541535Z",
     "start_time": "2021-06-28T16:01:50.123395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boucle sur toutes les localisations pour obtenir leurs coordonn√©es\n",
    "\n",
    "coordinates = {'latitude': [], 'longitude': []}\n",
    "\n",
    "for user_loc in tqdm(locations):\n",
    "    try:\n",
    "        location = geolocator.geocode(user_loc)\n",
    "        coordinates['latitude'].append(location.latitude)\n",
    "        coordinates['longitude'].append(location.longitude)         \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-transfer",
   "metadata": {},
   "source": [
    "3. Transformer le dictionnaire de la question pr√©c√©dente en dataframe (de 2 colonnes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-terrace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:02:23.640412Z",
     "start_time": "2021-06-28T16:02:23.546723Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_loc = pd.DataFrame(coordinates)\n",
    "df_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-pasta",
   "metadata": {},
   "source": [
    "4. Utiliser la fonction ci-dessous pour afficher les localisations sur une carte interactive.\n",
    "\n",
    "_Note : La fonction utilise le package `folium` pour cr√©er la carte. Pas la peine de tout comprendre dans la fonction, il s'agit de d√©tails de syntaxe li√©s √† ce package ; en revanche il est int√©ressant de lire attentivement la \"docstring\", c'est-√†-dire  la description de la function, afin de comprendre quels param√®tres doivent √™tre fournis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-terminology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:04:57.556770Z",
     "start_time": "2021-06-28T16:04:57.536283Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_locations(df, lat, lon, kind='heatmap', save=False, zoom=2):\n",
    "    \"\"\"\n",
    "    Displays a map with 2 layers (OSM and ESRI satellite imagery) and a heatmap or markers\n",
    "    corresponding to the locations passed as a dataframe of latitudes and longitudes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df: dataframe\n",
    "        A dataframe containing at least 2 columns, corresponding to the latitude and \n",
    "        longitudes of the points you want to plot.\n",
    "        \n",
    "    lat: string\n",
    "        The name of the column containing the latitudes.\n",
    "    \n",
    "    lon: string\n",
    "        The name of the column containing the longitudes.\n",
    "    \n",
    "    kind: string, either 'heatmap' (default) or 'markers'\n",
    "        The kind of visualization you want to plot.\n",
    "    \n",
    "    save: boolean, optional, default: False\n",
    "        Save the map as 'map.html' in the current folder.\n",
    "        \n",
    "    zoom: int, between 0 and 20, optional, default: 2\n",
    "        The zoom of the base map. Lower is less zoomed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create map, with default OSM tile layer\n",
    "    m = folium.Map(location=[df[lat].mean(), df[lon].mean()], zoom_start=zoom)\n",
    "\n",
    "    # Create markers\n",
    "    locations = []\n",
    "    popups = []\n",
    "    for idx, row in df.iterrows():\n",
    "        locations.append([row[lat], row[lon]])\n",
    "        popups.append(([row[lat], row[lon]]))\n",
    "\n",
    "    # Add markers\n",
    "    if kind == 'markers':\n",
    "        s = folium.FeatureGroup(name='Points')\n",
    "        s.add_child(MarkerCluster(locations=locations, popups=popups))\n",
    "        m.add_child(s)\n",
    "\n",
    "    # Plot heatmap\n",
    "    elif kind == 'heatmap':\n",
    "        heatmap = df[[lat, lon]].values\n",
    "        m.add_child(HeatMap(heatmap, name='Heatmap', radius=12))\n",
    "    else:\n",
    "        print('Please specify a valid kind of map (\"markers\" or \"heatmap\").')\n",
    "    \n",
    "    # Add ability to see lat and lon onclick\n",
    "    m.add_child(folium.LatLngPopup())\n",
    "\n",
    "    # Add satellite layer\n",
    "    tile = folium.TileLayer(\n",
    "            tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "            attr = 'Esri',\n",
    "            name = 'Esri Satellite',\n",
    "            overlay = False,\n",
    "            control = True\n",
    "           ).add_to(m)\n",
    "\n",
    "    # Add layer control toggle\n",
    "    # We can control the display of both tile layers and overlay figures\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    # Save map as HTML\n",
    "    if save:\n",
    "        m.save(\"map.html\")\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-innocent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:04:59.150541Z",
     "start_time": "2021-06-28T16:04:58.306991Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_locations(df_loc, 'latitude', 'longitude', kind='markers', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-senate",
   "metadata": {},
   "source": [
    "#### R√©sutats finaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-cowboy",
   "metadata": {},
   "source": [
    "Comme r√©sultat final de cette analyse, nous voulons une **liste** contenant les principaux r√©sultats de notre analyse.\n",
    "\n",
    "Nous voulons que tout soit dans une liste unique, afin de pouvoir la **sauvegarder** puis utiliser l'API Google Sheets (section suivante) pour l'envoyer directement dans un **spreadsheet**, o√π elle constituera une **nouvelle ligne**.\n",
    "\n",
    "De cette fa√ßon, nous serons en mesure d'ex√©cuter l'analyse √† nouveau √† intervalles r√©guliers, et d'ajouter simplement √† chaque fois une nouvelle ligne de r√©sultats √† notre Google Sheets, afin de suivre l'√©volution de notre sujet au fil du temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-publisher",
   "metadata": {},
   "source": [
    "1. Cr√©er une liste finale contenant 13 √©l√©ments (dans cet ordre) :\n",
    "\n",
    "* la date de l'analyse (aujourd'hui), sous forme de string\n",
    "* le nombre de tweets que nous avons analys√©s\n",
    "* les 10 mots les plus fr√©quents sous forme de liste\n",
    "* les 10 emojis les plus fr√©quents sous forme de liste\n",
    "* les 10 hashtags les plus fr√©quents sous forme de liste\n",
    "* le sentiment moyen de nos tweets (score compound)\n",
    "* le nombre moyen de followers\n",
    "* le nombre moyen de statuts\n",
    "* le nombre moyen de retweets\n",
    "* le nombre moyen de favoris\n",
    "* le tweet le plus populaire\n",
    "* la liste de latitudes\n",
    "* la liste de longitudes\n",
    "\n",
    "_Indices : Nous avons d√©j√† tout calcul√© ici, √† l'exception de la date d'aujourd'hui ! Vous devez juste rassembler tous les r√©sultats pr√©c√©dents (et √©ventuellement ajuster leur format). Pour la date d'aujourd'hui, vous pouvez utiliser `import datetime` puis `str(datetime.datetime.now())`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-bible",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T19:20:01.793242Z",
     "start_time": "2021-04-21T19:20:01.768029Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-laser",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T19:20:21.086335Z",
     "start_time": "2021-04-21T19:20:21.059411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_insights = [str(datetime.datetime.now()),\n",
    "                  len(df),\n",
    "                  [t[0] for t in top10_words],\n",
    "                  [t[0] for t in top10_emojis],\n",
    "                  [t[0] for t in top10_hashtags],\n",
    "                  mean_sentiment,\n",
    "                  mean_followers,\n",
    "                  mean_statuses,\n",
    "                  mean_retweets,\n",
    "                  mean_favorites,\n",
    "                  top_tweet,\n",
    "                  coordinates['latitude'],\n",
    "                  coordinates['longitude'],\n",
    "                 ]\n",
    "final_insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-brooklyn",
   "metadata": {},
   "source": [
    "2. Convertir la liste finale en string et la sauvegarder sous forme de fichier texte nomm√© `analysis.txt`.\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* Vous devrez utiliser la syntaxe `with open()`. Voici un exemple de cette syntaxe, √† adapter √† votre cas :\n",
    "\n",
    "```\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(content)\n",
    "```\n",
    "\n",
    "* Certains emojis peuvent provoquer des erreurs d'encoding lorsqu'ils sont sauvegard√©s dans un ficher texte. Si c'est le cas, vous pouvez utiliser la ligne suivante au moment de l'√©criture de votre fichier, afin de convertir les emojis en caract√®res unicode : `f.write(str(content).encode('unicode-escape').decode(\"utf-8\"))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-village",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T23:59:23.358577Z",
     "start_time": "2021-04-21T23:59:23.323671Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('analysis.txt', 'w') as f:\n",
    "    f.write(str(final_insights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4ccda",
   "metadata": {},
   "source": [
    "## API Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2bc79",
   "metadata": {},
   "source": [
    "Dans cette derni√®re section, vous serez guid√©s pour la derni√®re √©tape de ce projet Twitter : **envoyer les r√©sultats de l'anayse dans un fichier Google Sheets**. L'objectif est de construire la brique finale de notre workflow, afin d'avoir un **code complet pr√™t √† √™tre automatis√© ou r√©utilis√© sur d'autres sujets**. \n",
    "\n",
    "Nous avons choisi Google Sheets car c'est un format facilement partageable, compris par tous les utilisateurs (m√™me non-tech), et surtout parce que vous pouvez facilement l'utiliser pour **construire un dashboard de suivi** ‚Äî par exemple avec **Data Studio**, un dashboard qui serait automatiquement mis √† jour chaque semaine pour suivre visuellement la perception de votre sujet au fil du temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f762dd8",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd21211",
   "metadata": {},
   "source": [
    "Nous avons d'abord besoin de connecter ce notebook √† votre **compte de service Google Cloud** (utilisez le PDF d'instructions disponible sur la plateforme cr√©er un compte si besoin) gr√¢ce au package **`gspread`**.\n",
    "\n",
    "Pour vous authentifier, copiez-collez ci-dessous le **chemin de la cl√©** (fichier .json) qui a √©t√© t√©l√©charg√©e au moment de la cr√©ation de votre compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fe7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre cl√© devrait ressembler √† ceci :\n",
    "path = \"/Users/Thomas/Downloads/databird-309423-c4ff87ee5aec.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentification avec gspread\n",
    "gc = gspread.service_account(filename=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa72d67",
   "metadata": {},
   "source": [
    "### Chargement des r√©sultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398160c",
   "metadata": {},
   "source": [
    "1. Lire le fichier `analysis.txt` qui a √©t√© sauvegard√© √† la fin de l'analyse Twitter.\n",
    "\n",
    "_Indice : Vous pouvez ouvrir le fichier avec la fonction `open()`, puis le lire avec la m√©thode `.read()`. Ci-dessous, un exemple de syntaxe, √† adapter √† votre cas :_\n",
    "\n",
    "```\n",
    "f = open(filepath, 'r')\n",
    "f = f.read()\n",
    "f\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path = '/Users/Thomas/Documents/Data Science X2/DataBird/Batch4-5/3. Python/J9 APIs 1/Cas Twitter/analysis.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ad769",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(analysis_path, \"r\")\n",
    "file = file.read()\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f3c8b",
   "metadata": {},
   "source": [
    "2. Quel est le type du fichier que vous venez de lire (type python) ? Convertissez-le en liste.\n",
    "\n",
    "_Indice : Vous aurez besoin de la fonction `ast.literal_eval()`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94391b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "row = ast.literal_eval(file)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53551753",
   "metadata": {},
   "source": [
    "3. La liste contient diff√©rents types de donn√©es (string, int, dictionnaire, liste). Hors Google Sheets n'acceptera pas de mettre un dictionnaire ou une liste dans une cellule (c'est comme dans Excel : les seuls types de donn√©es que vous pouvez avoir dans une cellule sont du texte ou des chiffres, ou tout au plus des dates). Comment r√©soudriez-vous ce probl√®me ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir tous les √©l√©ments non-num√©riques de la liste en strings\n",
    "formatted_row = [str(x) if not (type(x) == int or type(x) == float) else x for x in row]\n",
    "formatted_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb04a3",
   "metadata": {},
   "source": [
    "### Envoi au spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219ef29",
   "metadata": {},
   "source": [
    "Dans cette section, nous ouvrons une feuille de calcul Google Sheets avec `gspread` et nous y ajoutons une nouvelle ligne contenant la liste de r√©sultats. Assurez-vous de bien **cr√©er d'abord cette feuille de calcul** et **d'autoriser l'API Google Sheets**, en suivant les **instructions** disponibles sur la plateforme Databird (fichier PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66757672",
   "metadata": {},
   "source": [
    "1. Ouvrez votre spreadsheet avec `gspread` (aidez-vous du notebook de d√©mo si besoin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ecc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet = gc.open_by_key('1c_Eij3Y-fAX7qaOQKSFOBivtV5Zb3oNprY3qE_PiNZU').sheet1\n",
    "worksheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb40188",
   "metadata": {},
   "source": [
    "2. Ajoutez les r√©sultats de l'analyse Twitter (c'est-√†-dire la liste) en tant que nouvelle ligne de votre feuille de calcul (√† nouveau, aidez-vous de la d√©mo pour utiliser la m√©thode la plus ad√©quate !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet.append_row(formatted_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a727a",
   "metadata": {},
   "source": [
    "[BONUS] 3. Ecrivez une longue fonction qui reprend toutes les √©tapes du projet (de la recherche initiale √† l'envoi dans Google Sheets) pour pouvoir appliquer le workflow complet sur une nouvelle recherche de tweets (√† passer en argument de la fonction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import emoji\n",
    "import gspread\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def twitter_analysis(query_params={'q':'tesla',\n",
    "                                   'lang':'en',\n",
    "                                   'since':\"2021-04-16\",\n",
    "                                   'until':\"2021-04-17\"}, \n",
    "                     full_output=False,\n",
    "                    ):\n",
    "    \n",
    "    print('Searching for tweets...')\n",
    "    \n",
    "    # Twitter API keys\n",
    "    API_KEY = 'diVvpJ68bD2T9Z7oSWcqRFSeh'\n",
    "    API_SECRET_KEY = 'A8WSmaeMahMkUDTI1oG8Q0OSaBCOswxCMCCK4JfkmyzsVBcE0R'\n",
    "    BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAJXNOgEAAAAAtUAkIFZgT38EsEGEMA%2BWXWZU3n4%3Daa9H6Jd9k7ddlaC92144L55mOqP9Nl4JBHhYLZyufTC50RNbxE'\n",
    "    \n",
    "    # Twitter authentication\n",
    "    auth = tweepy.AppAuthHandler(API_KEY, API_SECRET_KEY)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    # Twitter search\n",
    "    tweets = tweepy.Cursor(api.search,\n",
    "                           tweet_mode=\"extended\",\n",
    "                           **query_params,\n",
    "                          ).items(50)\n",
    "    tweets = list(tweets)\n",
    "    \n",
    "    print('Analyzing tweets...')\n",
    "    \n",
    "    # Information extraction\n",
    "    tweets_infos = []\n",
    "    for tweet in tqdm(tweets):\n",
    "        tweet_info = [tweet.full_text, \n",
    "                        tweet.created_at, \n",
    "                        [h['text'] for h in tweet.entities['hashtags']], \n",
    "                        tweet.user.location, \n",
    "                        tweet.user.followers_count, \n",
    "                        tweet.user.statuses_count, \n",
    "                        tweet.retweet_count, \n",
    "                        tweet.favorite_count] \n",
    "        tweets_infos.append(tweet_info)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    df = pd.DataFrame(tweets_infos)\n",
    "    df.columns = ['full_text', 'created_at', 'hashtags', 'location',\n",
    "                  'followers', 'statuses', 'retweets', 'favorites']\n",
    "    df['clean_text'] = df['full_text'].apply(lambda x: re.sub(\"(http\\S+)|(@\\S+)\", \"\", x).replace('#', '').lower())\n",
    "    \n",
    "    # Tweet cleaning / corpus creation\n",
    "    tweets_text = [t[0] for t in tweets_infos]\n",
    "    corpus = []\n",
    "    for tweet in tweets_text:\n",
    "        clean = re.sub(\"(http\\S+)|(@\\S+)\", \"\", tweet).replace('#', '').lower()\n",
    "        clean = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', clean)\n",
    "        clean = clean.split()\n",
    "        collection_words = ['amazon']\n",
    "        clean = [word for word in clean if not word in collection_words]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        clean = [word for word in clean if not word in stop_words]\n",
    "        corpus.extend(clean)\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    top10_words = Counter(corpus).most_common(10)\n",
    "\n",
    "    # Sentiment analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    clean_tweets = df['clean_text'].values\n",
    "    sentiment_scores = [sia.polarity_scores(t)[\"compound\"] for t in clean_tweets]\n",
    "    mean_sentiment = np.mean(sentiment_scores)\n",
    "    \n",
    "    # Hashtags analysis\n",
    "    hashtags = list(df['hashtags'].values)\n",
    "    hashtags = list(itertools.chain(*hashtags))\n",
    "    hashtags = [h.lower() for h in hashtags]\n",
    "    top10_hashtags = Counter(hashtags).most_common(10)\n",
    "    \n",
    "    # Emojis analysis\n",
    "    emoji_corpus = []\n",
    "    for word in corpus:\n",
    "        for character in word:\n",
    "            if character in emoji.UNICODE_EMOJI['en']:\n",
    "                emoji_corpus.append(character)\n",
    "    top10_emojis = Counter(emoji_corpus).most_common(10)\n",
    "    \n",
    "    # Popularity analysis\n",
    "    mean_followers = df['followers'].mean()\n",
    "    mean_statuses = df['statuses'].mean()\n",
    "    mean_retweets = df['retweets'].mean()\n",
    "    mean_favorites = df['favorites'].mean()\n",
    "    df['sum_favorites_retweets'] = df['favorites'] + df['retweets']\n",
    "    top_tweet = df[df['sum_favorites_retweets'] == max(df['sum_favorites_retweets'])]\n",
    "    top_tweet = top_tweet['full_text'].values[0]\n",
    "    df = df.drop(columns=['sum_favorites_retweets'])\n",
    "    \n",
    "    # Location coordinates\n",
    "    locations = df.location.values\n",
    "    geolocator = Nominatim(user_agent=\"my-app\")\n",
    "    coordinates = {'latitude': [], 'longitude': []}\n",
    "    for user_loc in tqdm(locations):\n",
    "        try:\n",
    "            location = geolocator.geocode(user_loc)\n",
    "            if location:\n",
    "                coordinates['latitude'].append(location.latitude)\n",
    "                coordinates['longitude'].append(location.longitude)         \n",
    "        except:\n",
    "            pass\n",
    "    df_loc = pd.DataFrame(coordinates)\n",
    "    \n",
    "    # Final output\n",
    "    final_insights = [str(datetime.datetime.now()),\n",
    "                  len(df),\n",
    "                  [t[0] for t in top10_words],\n",
    "                  [t[0] for t in top10_emojis],\n",
    "                  [t[0] for t in top10_hashtags],\n",
    "                  mean_sentiment,\n",
    "                  mean_followers,\n",
    "                  mean_statuses,\n",
    "                  mean_retweets,\n",
    "                  mean_favorites,\n",
    "                  top_tweet,\n",
    "                  coordinates['latitude'],\n",
    "                  coordinates['longitude'],\n",
    "                 ]\n",
    "    \n",
    "    print('Sending to Google Sheets...')\n",
    "    \n",
    "    # Google Sheets authentication\n",
    "    path = \"/Users/Thomas/Downloads/databird-309423-c4ff87ee5aec.json\"\n",
    "    gc = gspread.service_account(filename=path)\n",
    "        \n",
    "    # Prepare final ouput for sending\n",
    "    formatted_row = [str(x) if not (type(x) == int or type(x) == float) else x for x in final_insights]\n",
    "    \n",
    "    # Send to Google sheets\n",
    "    worksheet = gc.open_by_key('1c_Eij3Y-fAX7qaOQKSFOBivtV5Zb3oNprY3qE_PiNZU').sheet1\n",
    "    worksheet.append_row(formatted_row)\n",
    "    \n",
    "    print('Done!')\n",
    "\n",
    "    if full_output:\n",
    "        return final_insights, df\n",
    "    else:\n",
    "        return final_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_insights, full_output = twitter_analysis(query_params={'q':'tesla',\n",
    "                                                             'lang':'en',\n",
    "                                                             'since':\"2021-04-16\",\n",
    "                                                             'until':\"2021-04-17\"},\n",
    "                                               full_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72722329",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5dcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64c49c",
   "metadata": {},
   "source": [
    "F√©licitations, vous disposez maintenant d'un tr√®s bon outil d'analyse ! Vous pourriez placer le script complet dans un **fichier python .py** et l'ex√©cuter p√©riodiquement (par exemple chaque semaine), gr√¢ce √† un **planificateur** de script comme **Airflow** (le plus connu). Vous auriez bient√¥t plusieurs lignes dans votre Google Sheets, que vous pourriez directement utiliser pour **construire et partager des dashboards d'analyse** (gr√¢ce √† Data Studio) avec toute votre entreprise !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a298c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
