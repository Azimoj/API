{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supreme-diamond",
   "metadata": {},
   "source": [
    "# Analyse d'e-réputation Twitter — Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-matrix",
   "metadata": {},
   "source": [
    "Ce projet a pour objectif de récapituler (quasi) toutes les compétences vues dans cette formation pour analyser une e-réputation Twitter, en 3 temps :\n",
    "* Recherche des tweets avec l'API Twitter\n",
    "* Nettoyage (pre-processing) et analyse des tweets\n",
    "* Envoi dans un spreadsheet avec l'API Google Sheets.\n",
    "___\n",
    "\n",
    "Vous êtes data analyst chez Olist, et vous aimeriez savoir comment vos concurrents sont perçus par le public. Vous codez donc avec Python un programme qui utilise l'API Twitter pour **rechercher les tweets qui évoquent vos concurrents** (par exemple Amazon). Le programme ensuite les **nettoie**, les **analyse** (mots les plus courants, émotions les plus présentes, répartition géographique, sous forme de graphes et de statistiques) et enfin envoie les résultats de ces analyses dans un **spreadsheet**. \n",
    "\n",
    "Ce genre d'analyse peut ensuite être **répété**, par exemple chaque semaine, afin de surveiller l'état du marché et **l'évolution de la perception** que les clients ont de vos concurrents (ou de votre entreprise elle-même !). Le projet peut s'adapter à de nombreux usages !\n",
    "\n",
    "Dans ce notebook, vous serez guidé dans **toutes les étapes** de la création de cet outil. Nous utiliserons les API de Twitter et de Google Sheets, vous aurez donc besoin de **configurer un compte Google Cloud et un compte Twitter Developer** en suivant les instructions disponibles sur la plateforme Databird (**2 fichiers PDF**). Allons-y !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informational-diving",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:54:14.961741Z",
     "start_time": "2021-09-17T16:53:14.311413Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "Collecting oauthlib<4,>=3.2.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting requests-oauthlib<2,>=1.2.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting requests<3,>=2.27.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Installing collected packages: requests, oauthlib, requests-oauthlib, tweepy\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "Successfully installed oauthlib-3.2.2 requests-2.31.0 requests-oauthlib-1.3.1 tweepy-4.14.0\n",
      "Collecting gspread\n",
      "  Downloading gspread-5.11.2-py3-none-any.whl (46 kB)\n",
      "Collecting google-auth>=1.12.0\n",
      "  Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from google-auth>=1.12.0->gspread) (5.0.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (1.26.7)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, gspread\n",
      "Successfully installed google-auth-2.23.2 google-auth-oauthlib-1.1.0 gspread-5.11.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 rsa-4.9\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (4.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (0.5.0)\n",
      "Collecting httplib2>=0.9.1\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: six>=1.6.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from oauth2client) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from httplib2>=0.9.1->oauth2client) (3.0.4)\n",
      "Installing collected packages: httplib2, oauth2client\n",
      "Successfully installed httplib2-0.22.0 oauth2client-4.1.3\n",
      "Requirement already satisfied: nltk in c:\\users\\azade\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\azade\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\azade\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.8.0\n",
      "Requirement already satisfied: wordcloud in c:\\users\\azade\\anaconda3\\lib\\site-packages (1.9.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\azade\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\azade\\anaconda3\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\azade\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Collecting geopy\n",
      "  Downloading geopy-2.4.0-py3-none-any.whl (125 kB)\n",
      "Collecting geographiclib<3,>=1.52\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.0\n",
      "Collecting folium\n",
      "  Downloading folium-0.14.0-py2.py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\azade\\anaconda3\\lib\\site-packages (from folium) (2.31.0)\n",
      "Requirement already satisfied: jinja2>=2.9 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from folium) (2.11.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\azade\\anaconda3\\lib\\site-packages (from folium) (1.20.3)\n",
      "Collecting branca>=0.6.0\n",
      "  Downloading branca-0.6.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from jinja2>=2.9->folium) (1.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\azade\\anaconda3\\lib\\site-packages (from requests->folium) (1.26.7)\n",
      "Installing collected packages: branca, folium\n",
      "Successfully installed branca-0.6.0 folium-0.14.0\n"
     ]
    }
   ],
   "source": [
    "# exécuter cette cellule si tous les packages ne sont pas installés\n",
    "!pip install tweepy\n",
    "!pip install gspread \n",
    "!pip install oauth2client\n",
    "!pip install nltk\n",
    "!pip install emoji\n",
    "!pip install wordcloud\n",
    "!pip install geopy\n",
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-desire",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:55:01.151147Z",
     "start_time": "2021-09-17T16:54:43.823969Z"
    }
   },
   "outputs": [],
   "source": [
    "# packages généralistes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "# packages d'analyse de texte\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import emoji\n",
    "\n",
    "# packages graphiques\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, HeatMap\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# API wrappers Twitter et Google Sheets\n",
    "import tweepy\n",
    "import gspread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-galaxy",
   "metadata": {},
   "source": [
    "## API Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-cotton",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T23:30:25.888239Z",
     "start_time": "2021-04-18T23:30:25.871579Z"
    }
   },
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-theory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:54:31.746628Z",
     "start_time": "2021-09-17T16:54:31.703225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remplacez par vos propres clés d'API Twitter\n",
    "API_KEY = 'diVvpJ68bD2T9Z7oSWcqRFSeh'\n",
    "API_SECRET_KEY = 'A8WSmaeMahMkUDTI1oG8Q0OSaBCOswxCMCCK4JfkmyzsVBcE0R'\n",
    "BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAJXNOgEAAAAAtUAkIFZgT38EsEGEMA%2BWXWZU3n4%3Daa9H6Jd9k7ddlaC92144L55mOqP9Nl4JBHhYLZyufTC50RNbxE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-beauty",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:55:01.878474Z",
     "start_time": "2021-09-17T16:55:01.155666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Authentification et création du client d'API\n",
    "auth = tweepy.AppAuthHandler(API_KEY, API_SECRET_KEY)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-spell",
   "metadata": {},
   "source": [
    "### Recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-craft",
   "metadata": {},
   "source": [
    "1. **Recherchez les tweets à propos d'Amazon** (ou de **Tesla** — souvent plus amusants, d'expérience). Vous pouvez ajouter d'autres termes de recherche dans votre requête et ajuster les paramètres comme vous le souhaitez. Pour l’instant, limitez votre recherche à **100 tweets**.\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* Utilisez la fonction `tweepy.Cursor` vue dans le live-coding.\n",
    "* Lorsque vous recherchez un mot, l’API Twitter le recherche dans toutes données liées à un tweet (texte du tweet, hashtags, nom d’utilisateur, etc.).\n",
    "* Je vous conseille de filtrer les retweets avec `-filter:retweets` pour obtenir des tweets plus pertinents.\n",
    "* Le résultat de la fonction `tweepy.Cursor` est un \"générateur\". Convertissez ce résultat en liste à l’aide de la fonction `list()`.\n",
    "* Une fois convertis en liste, vous pouvez itérer sur les tweets pour en extraire des informations. Par exemple, pour extraire le texte des tweets, utilisez l’attribut `.full_text` sur les tweets dans votre boucle (voir live-coding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-syria",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:56:24.555012Z",
     "start_time": "2021-09-17T16:56:24.512851Z"
    }
   },
   "outputs": [],
   "source": [
    "query =\"tesla -filter:retweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-bronze",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:57:33.937595Z",
     "start_time": "2021-09-17T16:57:29.071355Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Recherche des tweets\n",
    "tweets = tweepy.Cursor(api.search,\n",
    "                       q=query,\n",
    "                       lang=\"en\",\n",
    "                       tweet_mode=\"extended\",\n",
    "                       #since=\"2021-06-18\", # put here the last week\n",
    "                       #until=\"2021-06-25\",\n",
    "                      ).items(50)\n",
    "tweets = list(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-little",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:58:16.746455Z",
     "start_time": "2021-09-17T16:58:16.414730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vérification : extraction du texte des tweets\n",
    "\n",
    "tweets_text = []\n",
    "for tweet in tqdm(tweets):\n",
    "    tweets_text.append(tweet.full_text)\n",
    "tweets_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-engine",
   "metadata": {},
   "source": [
    "2. Intéressons-nous seulement à **1 tweet** pour l'instant (un tweet complet, dans le résultat de `tweepy.Cursor`, pas seulement le texte d'un tweet). **Extraire les attributs les plus intéressants** de ce tweet à partir de la réponse au format **json**, et stocker ces informations dans une liste.\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* La réponse json fournie pour chaque tweet dans les résultats contient beaucoup plus d’informations que le simple texte du tweet. Commencez par extraire la réponse json complète d'un seul tweet avec `._json`, pour comprendre comment les informations sont organisées.\n",
    "* Puis utilisez la syntaxe `.attribut` (`.full_text`, `.user.screen_name`, etc.) pour extraire le texte du tweet, sa date de création, ses hashtags, la localisation de l'utilisateur, le nombre de followers de l'utilisateur, le nombre de statuts publiés par l'utilisateur, le nombre de retweets et le nombre de favoris du tweet.\n",
    "* Stockez toutes ces informations dans une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-savings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T16:59:19.962609Z",
     "start_time": "2021-09-17T16:59:19.843976Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Réponse json complète pour 1 tweet\n",
    "\n",
    "test = list(tweets)[10]\n",
    "json_test = test._json\n",
    "json_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-sussex",
   "metadata": {},
   "source": [
    "On souhaite extraire les attributs : `full_text`, `created_at`, `entities.hashtags`, `user.location`, `user.followers_count`, `user.statuses_count`, `retweet_count`, `favorite_count` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-think",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:07:50.682472Z",
     "start_time": "2021-09-17T17:07:50.646927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extraction des informations pertinentes\n",
    "\n",
    "info = [test.full_text, \n",
    "        test.created_at,\n",
    "        #test.entities['hashtags'], # ne fonctionne pas en l'état ! \n",
    "        [h['text'] for h in test.entities['hashtags']], # solution détaillée dessous\n",
    "        test.user.location, \n",
    "        test.user.followers_count, \n",
    "        test.user.statuses_count, \n",
    "        test.retweet_count, \n",
    "        test.favorite_count]\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-performer",
   "metadata": {},
   "source": [
    "Ci-dessous, les quelques tests effectués afin d'écrire la liste compréhension permettant d'extraire les hashtags :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-respect",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:01:16.566008Z",
     "start_time": "2021-09-17T17:01:16.526378Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experience = [{'text': 'dogecoin', 'indices': [39, 48]},\n",
    "  {'text': 'doge', 'indices': [49, 54]}]\n",
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-tribune",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:02:26.267982Z",
     "start_time": "2021-09-17T17:02:26.223657Z"
    }
   },
   "outputs": [],
   "source": [
    "hashtags = []\n",
    "for x in experience:\n",
    "    hashtag = x['text']\n",
    "    hashtags.append(hashtag)\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-entertainment",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:02:30.871999Z",
     "start_time": "2021-09-17T17:02:30.836773Z"
    }
   },
   "outputs": [],
   "source": [
    "[h['text'] for h in experience]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-knitting",
   "metadata": {},
   "source": [
    "3. Répliquez le même processus sur **tous les tweets** issus de la recherche.\n",
    "\n",
    "_Indices : Vous aurez probablement besoin d'une boucle. Vous pouvez utiliser **`tqdm`** pour afficher une barre de progression. L'objectif est d'obtenir une **liste de listes** (chaque sous-liste correspondant aux informations d'1 tweet)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-inflation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:03:57.031512Z",
     "start_time": "2021-09-17T17:03:56.792561Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extraction des informations pour tous les tweets\n",
    "\n",
    "tweets_infos = []\n",
    "for tweet in tqdm(tweets):\n",
    "    tweet_info = [tweet.full_text, \n",
    "                    tweet.created_at, \n",
    "                    [h['text'] for h in tweet.entities['hashtags']], \n",
    "                    tweet.user.location, \n",
    "                    tweet.user.followers_count, \n",
    "                    tweet.user.statuses_count, \n",
    "                    tweet.retweet_count, \n",
    "                    tweet.favorite_count] \n",
    "    tweets_infos.append(tweet_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-quebec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:04:01.423178Z",
     "start_time": "2021-09-17T17:04:01.363631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-heavy",
   "metadata": {},
   "source": [
    "4. Convertir les résultats en **dataframe** (une ligne par tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-blink",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:04:57.849464Z",
     "start_time": "2021-09-17T17:04:57.716305Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets_infos)\n",
    "df.columns = ['full_text', 'created_at', 'hashtags', 'location',\n",
    "              'followers', 'statuses', 'retweets', 'favorites']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-scale",
   "metadata": {},
   "source": [
    "### Nettoyage (pre-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-enclosure",
   "metadata": {},
   "source": [
    "On souhaite ici préparer le **texte** des tweets pour l'analyse. Une bonne pratique consiste à **tester toutes les étapes de pre-processing sur 1 seul tweet**, avant de les **appliquer à tous les tweets dans une boucle**. Et c'est ce que nous allons faire !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-judges",
   "metadata": {},
   "source": [
    "0. Isolez le texte d'un seul tweet pour pouvoir tester les étapes de pre-processing lors des questions suivantes.\n",
    "\n",
    "_Conseil : C'est encore mieux si votre tweet contient des @, # et des liens hypertexte, que nous nous efforcerons de supprimer. Si vous ne trouvez pas de tweet qui convient, écrivez simplement un faux tweet de test !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-ordering",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:11:04.133976Z",
     "start_time": "2021-09-17T17:11:04.038487Z"
    }
   },
   "outputs": [],
   "source": [
    "test = 'Come spend Christmas 👑 with Amazon! https://t.co/r8oG #Christmas @amazon'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-plastic",
   "metadata": {},
   "source": [
    "1. Supprimer les liens, identifications (caractère @ et nom d'utilisateur, peu utiles à l'analyse) et tags (caractère # seulement), grâce à la fonction `clean_tweet` ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-determination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:11:38.642005Z",
     "start_time": "2021-09-17T17:11:38.569260Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_tweet(txt):\n",
    "    return re.sub(\"(http\\S+)|(@\\S+)\", \"\", txt).replace('#', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-inclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:14:18.602288Z",
     "start_time": "2021-09-17T17:14:18.569047Z"
    }
   },
   "outputs": [],
   "source": [
    "test = clean_tweet(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-tuner",
   "metadata": {},
   "source": [
    "2. Mettre le texte en minuscules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-crown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:14:30.045914Z",
     "start_time": "2021-09-17T17:14:30.018610Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test.lower()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-clarity",
   "metadata": {},
   "source": [
    "3. Supprimer tous les signes de ponctuation et les remplacer par des espaces.\n",
    "\n",
    "_Conseils : Vous aurez sans doute besoin d'une \"regex\" (expression régulière), comme dans la fonction `clean_tweet`. Attention à ne pas supprimer les emojis. Vous pouvez utiliser la regex : `re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', test_tweet)`. Cette regex signifie : \"remplacer tous les caractères qui ne sont pas (`^`) des lettres (`\\w`), des espaces (`\\s`) ou des caractères unicode (`\\U`, sur tous les codes possibles) par un espace\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-atlantic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:15:54.326009Z",
     "start_time": "2021-09-17T17:15:54.283458Z"
    }
   },
   "outputs": [],
   "source": [
    "test = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', test)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-fetish",
   "metadata": {},
   "source": [
    "4. Scinder le texte en mots individuels et supprimer les espaces avant et après chaque mot (\"leading and trailing white spaces\") si besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-earth",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:16:42.301944Z",
     "start_time": "2021-09-17T17:16:42.260114Z"
    }
   },
   "outputs": [],
   "source": [
    "test = test.split()\n",
    "test  # .strip() à appliquer sur tous les mots si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-victim",
   "metadata": {},
   "source": [
    "5. Supprimer les mots de la requête (\"collection words\").\n",
    "\n",
    "_Indice : Vous pouvez utiliser une list comprehension incluant une condition._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-glory",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:18:13.788825Z",
     "start_time": "2021-09-17T17:18:13.772846Z"
    }
   },
   "outputs": [],
   "source": [
    "collection_words = ['amazon']\n",
    "test = [ word for word in test if not word in collection_words ]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-stephen",
   "metadata": {},
   "source": [
    "6. Supprimer les \"stopwords\", c'est-à-dire les mots courants qui ne portent pas de sens (\"the\", \"a\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-opinion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:18:47.161285Z",
     "start_time": "2021-09-17T17:18:47.115488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Télécharger la liste de stopwords du package nltk \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(list(stop_words)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-tonight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:19:03.296435Z",
     "start_time": "2021-09-17T17:19:03.287990Z"
    }
   },
   "outputs": [],
   "source": [
    "test = [ word for word in test if not word in stop_words ]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-summit",
   "metadata": {},
   "source": [
    "7. Appliquer les étapes précédentes à tous les tweets. L'objectif est d'obtenir un long corpus de texte, i.e. une longue liste contenant tous les mots indicviduels issus des tweets nettoyés.\n",
    "\n",
    "_Conseils :_\n",
    "* Commencez par créer une liste contentant le texte de tous vos tweets.\n",
    "* Utilisez ensuite une boucle pour appliquer les étapes de nettoyage à chaque tweet.\n",
    "* Souvenez-vous de la différence entre `.append` et `.extend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-break",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:19:51.287217Z",
     "start_time": "2021-09-17T17:19:51.276082Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A partir du résultat de la recherche, conserver seulement le texte des tweets\n",
    "tweets_text = [t[0] for t in tweets_infos]\n",
    "tweets_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-stomach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:21:39.997453Z",
     "start_time": "2021-09-17T17:21:39.883454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construire le corpus de mots en appliquant les étapes testées plus haut\n",
    "corpus = []\n",
    "for tweet in tweets_text:\n",
    "    clean = clean_tweet(tweet).lower()\n",
    "    clean = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', clean)\n",
    "    clean = clean.split()\n",
    "    collection_words = ['tesla']\n",
    "    clean = [word for word in clean if not word in collection_words]\n",
    "    clean = [word for word in clean if not word in stop_words]\n",
    "    corpus.extend(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-business",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:21:41.752114Z",
     "start_time": "2021-09-17T17:21:41.698493Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-intro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:22:24.128258Z",
     "start_time": "2021-09-17T17:22:24.102164Z"
    }
   },
   "outputs": [],
   "source": [
    "# Version fonctionnelle\n",
    "\n",
    "def clean_full(tweet: str):\n",
    "    clean = clean_tweet(tweet).lower()\n",
    "    clean = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', clean)\n",
    "    clean = clean.split()\n",
    "    collection_words = ['amazon']\n",
    "    clean = [word for word in clean if not word in collection_words]\n",
    "    clean = [word for word in clean if not word in stop_words]\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-grace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:22:38.351516Z",
     "start_time": "2021-09-17T17:22:38.339258Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for tweet in tweets_text:\n",
    "    result = clean_full(tweet)\n",
    "    corpus.extend(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-liechtenstein",
   "metadata": {},
   "source": [
    "8. Excellent, nous avons désormais un corpus de mots prêt à l'analyse ! Pour cette dernière question de la section, créer un nouvelle colonne dans le dataframe de la section précédente. Mettre dans cette colonne, le text des tweets, en leur appliquant uniquement les étapes 1 and 2 du cleaning (supprimer les @, # et les links, et mettre en minuscules). Nommer cette colonne `clean_text`. Nous l'utilserons aussi pour l'analyse.\n",
    "\n",
    "_Conseil : Vous pouvez utiliser `apply`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-shadow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:23:18.632061Z",
     "start_time": "2021-09-17T17:23:18.570955Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['full_text'].apply(lambda x: clean_tweet(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-upset",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-17T17:23:21.726462Z",
     "start_time": "2021-09-17T17:23:21.594880Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-barrier",
   "metadata": {},
   "source": [
    "### Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-problem",
   "metadata": {},
   "source": [
    "Dans cette section, nous mènerons plusieurs analyses fondées sur le corpus de mots et sur le dataframe contenant les tweets complets nettoyés, ainsi que les autres informations issues de notre recherche."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-cannon",
   "metadata": {},
   "source": [
    "#### Mots les plus fréquents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-spine",
   "metadata": {},
   "source": [
    "0. Compter le nombre de mots uniques dans le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-witness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:12:08.757199Z",
     "start_time": "2021-06-28T15:12:08.751562Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"There are {len(set(corpus))} words in the combination of all searched tweets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-pharmacology",
   "metadata": {},
   "source": [
    "1. Compter le nombre d'occurences de chaque mot. Quels sont les 10 mots les plus fréquents ?\n",
    "\n",
    "_Indices : Souvenez-vous de `Counter()` du package `collections`. Essayez-aussi d'appliquer `.most_common(10)` au résultat de votre compteur._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-coral",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:16:05.681320Z",
     "start_time": "2021-06-28T15:16:05.649398Z"
    }
   },
   "outputs": [],
   "source": [
    "top10_words = Counter(corpus).most_common(10)\n",
    "top10_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-institution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:16:28.760420Z",
     "start_time": "2021-06-28T15:16:28.743046Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(corpus).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-stevens",
   "metadata": {},
   "source": [
    "2. Afficher la fréquence des n premiers mots du corpus (n doit être modifiable) dans un diagramme en bâtons (bar plot).\n",
    "\n",
    "_Indice : Vous pouvez créer le bar plot manuellement, ou bien utiliser la fonction du notebook solutions (plus rapide !)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-legend",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:20:21.889725Z",
     "start_time": "2021-06-28T15:20:21.877548Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_frequencies(words, top=15):\n",
    "    \n",
    "    # Create dataframe for plotting\n",
    "    frequency_df = pd.DataFrame(Counter(words).most_common(top),\n",
    "                             columns=['words', 'count'])\n",
    "    \n",
    "    # Plot\n",
    "    frequency_df.sort_values(by='count').plot.barh(x='words',\n",
    "                                                   y='count',\n",
    "                                                   color=\"red\")\n",
    "    plt.title(\"Most Common Words in Tweets\", fontweight='semibold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-survivor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:20:23.442983Z",
     "start_time": "2021-06-28T15:20:22.736545Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_frequencies(corpus, top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-edmonton",
   "metadata": {},
   "source": [
    "3. Utiliser la fonction ci-dessous pour afficher un nuage de mots (wordcloud) des mots les plus communs dans le corpus.\n",
    "\n",
    "_Note : Vous pouvez inclure un argument `mask` dans la fonction, avec le chemin d'une image qui donnera forme au nuage de mots. Il peut s'agir de (quasi) n'importe quelle image en noir et blanc — en particulier, vous pouvez utiliser le fichier `glasses.jpg` disponible sur la plateforme._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-batch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:24:17.227971Z",
     "start_time": "2021-06-28T15:24:17.215993Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordcloud(corpus, title=None, mask=None, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Draws a word cloud with an optional mask shape, using the wordcloud package.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    corpus: list\n",
    "        List of all individual words to be used for the word cloud.\n",
    "    \n",
    "    title: string, optional, default: None\n",
    "        The title of your word cloud.\n",
    "\n",
    "    mask: string, optional, default: None\n",
    "        The path to the image to be used as mask. It must be binary (black & white).\n",
    "        \n",
    "    figsize: tuple, optional, default: (10,10)\n",
    "        The size of the chart area.\n",
    "    \"\"\"\n",
    "    \n",
    "    # preprocess data\n",
    "    corpus = str(corpus).replace(\"'\", \"\")\n",
    "    \n",
    "    # load mask if provided\n",
    "    if mask != None:\n",
    "        mask_ = Image.open(mask)\n",
    "        fn = lambda x : 255 if x >= 200 else 0\n",
    "        mask_ = mask_.convert('L').point(fn, mode='1').convert('RGB')\n",
    "        mask_ = np.array(mask_)\n",
    "    else: \n",
    "        mask_ = None\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        mask = mask_,\n",
    "        max_font_size = 45,\n",
    "        min_font_size = 5,\n",
    "        contour_width = 0.1,\n",
    "        contour_color = 'silver',\n",
    "        repeat = True,\n",
    "        stopwords=STOPWORDS,\n",
    "        random_state = 1).generate(str(corpus))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=figsize)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=18, y=0.75)\n",
    "    plt.imshow(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-stuart",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:24:18.746873Z",
     "start_time": "2021-06-28T15:24:17.900185Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud(corpus, title=\"Most Common Words in Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-mainland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:24:52.553134Z",
     "start_time": "2021-06-28T15:24:50.406007Z"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud(corpus, title=\"Most Common Words in Tweets\", mask='glasses.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-buffer",
   "metadata": {},
   "source": [
    "#### Hashtags et emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-entity",
   "metadata": {},
   "source": [
    "1. Quels sont les hashtags les plus fréquents dans nos tweets ? (top 10)\n",
    "\n",
    "_Indices : Vous aurez sans doute besoin \"d'aplatir\" (\"flatten\") une liste de listes, c'est-à-dire transformer une liste de listes en une seule longue liste. Pour cela, vous pouvez utiliser le package `itertools` : `simple_list = list(itertools.chain(*list_of_lists))`. Souvenez-vous aussi de mettre tous les hashtags en minuscules._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-accreditation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:45:51.375439Z",
     "start_time": "2021-06-28T15:45:51.324424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtags = list(df['hashtags'].values)\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-tablet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:48:26.358366Z",
     "start_time": "2021-06-28T15:48:26.348910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hashtags = list(itertools.chain(*hashtags))\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-lawyer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:49:38.411659Z",
     "start_time": "2021-06-28T15:49:38.402059Z"
    }
   },
   "outputs": [],
   "source": [
    "# mettre en minuscules pour éviter les doublons\n",
    "hashtags = [ h.lower() for h in hashtags ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-estonia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T18:23:02.540771Z",
     "start_time": "2021-04-21T18:23:02.532349Z"
    }
   },
   "outputs": [],
   "source": [
    "top10_hashtags = Counter(hashtags).most_common(10)\n",
    "top10_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-sense",
   "metadata": {},
   "source": [
    "2. Quels sont les emojis les plus fréquents ? (top 10)\n",
    "\n",
    "_Indice : La première étape pour cette question est de créer un corpus d'emojis en conservant seulement les emojis dans le corpus de mots complet. Vous pouvez pour cela utiliser la fonction `char_is_emoji` ci-dessous._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-display",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:38.523669Z",
     "start_time": "2021-06-28T15:51:38.518321Z"
    }
   },
   "outputs": [],
   "source": [
    "def char_is_emoji(character):\n",
    "    return character in emoji.UNICODE_EMOJI['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-account",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:39.125056Z",
     "start_time": "2021-06-28T15:51:39.110498Z"
    }
   },
   "outputs": [],
   "source": [
    "emoji_corpus = []\n",
    "for word in corpus:\n",
    "    for character in word:\n",
    "        if char_is_emoji(character):\n",
    "            emoji_corpus.append(character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-pledge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:39.828987Z",
     "start_time": "2021-06-28T15:51:39.804509Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emoji_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-shipping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:51:53.502596Z",
     "start_time": "2021-06-28T15:51:53.473177Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top10_emojis = Counter(emoji_corpus).most_common(10)\n",
    "top10_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2d7dd",
   "metadata": {},
   "source": [
    "#### Analyse de sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801dfaa",
   "metadata": {},
   "source": [
    "Parlons maintenant de **NLP, \"Natural Language Processing\"** (parfois traduit \"Traitement Automatique du Langage Naturel\", TALN). Le NLP est un champ de recherche du Deep Learning, l'apprentissage profond, qui s'intéresse à l'analyse de texte et à la construction de modèles prédictifs liés au langage. En particulier, un important domaine d'application du NLP est l'analyse de sentiment, i.e. **prédire le sentiment transmis par une phrase ou un paragraphe**. \n",
    "\n",
    "Nous allons utiliser l'analyse de sentiment sur nos tweets pour essayer de déterminer les **sentiments les plus communs quand les internautes évoquent notre sujet sur Twitter**.\n",
    "\n",
    "La construction d'un modèle de NLP est relativement complexe, heureusement le package `nltk` nous fournit plusieurs fonctions de NLP, notamment une **fonction pour faire appel à un modèle d'analyse de sentiment pré-entraîné**. Exécutez la cellule ci-dessous pour initialiser ce modèle, puis suivez les questions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e22ee3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:26:08.378674Z",
     "start_time": "2021-06-28T15:26:08.302895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisation du modèle d'analyse de sentiment\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0ba0d5",
   "metadata": {},
   "source": [
    "1. Prédire le sentiment d'un seul tweet avec la commande `sia.polarity_scores(test_tweet)`. Le tweet passé en argument doit être une phrase complète, i.e. une longue chaîne de caractères, et non une liste de mots.\n",
    "\n",
    "_Note : Comme vous le verrez, le sentiment est mesuré en pourcentage, réparti entre 'négatif', 'positif' et 'neutre'. La commande renvoie aussi un score composite ('compound'), compris entre -1 et 1. Des explications supplémentaires sur le modèle et le calcul des scores sont disponibles sur le GitHub du modèle Vader utilisé par nltk : [link](https://github.com/cjhutto/vaderSentiment#about-the-scoring)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946edcdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:26:36.108576Z",
     "start_time": "2021-06-28T15:26:36.103004Z"
    }
   },
   "outputs": [],
   "source": [
    "test = 'Come spend the Christmas period 👑 with Amazon !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d731aa28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:30:41.767037Z",
     "start_time": "2021-06-28T15:30:41.760093Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sia.polarity_scores(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa6727",
   "metadata": {},
   "source": [
    "2. Calculer le sentiment moyen de tous les tweets.\n",
    "\n",
    "_Indice : Utilisez la colonne `clean_text` de votre dataframe. Le score qui nous intéresse ici est le score \"compound\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235cde7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:34:37.593694Z",
     "start_time": "2021-06-28T15:34:37.548211Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_tweets = df['clean_text'].values\n",
    "clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea31d4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:36:17.356723Z",
     "start_time": "2021-06-28T15:36:17.315319Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiment_scores = [ sia.polarity_scores(t)[\"compound\"] for t in clean_tweets ]\n",
    "sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64018355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:36:45.151964Z",
     "start_time": "2021-06-28T15:36:45.118819Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_sentiment = np.mean(sentiment_scores)\n",
    "mean_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7da65f",
   "metadata": {},
   "source": [
    "3. Afficher la répartition des sentments sous forme de graphe (histogramme) grâce à la fonction ci-dessous.\n",
    "\n",
    "_Note : La majorité des tweets seront sans doute classés comme \"neutres\", car le modèle n'a pas été entraîné sur des données tout à fait similaires aux nôtres, et nos données ne sont pas préparées de manière tout à fait adéquate pour le modèle — les résultats ne sont donc qu'indicatifs, mais cette première approche du NLP me semble intéressante néanmoins !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359d1f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:39:15.115448Z",
     "start_time": "2021-06-28T15:39:15.097820Z"
    }
   },
   "outputs": [],
   "source": [
    "# sentiment analysis plot\n",
    "\n",
    "def plot_sentiments(tweets):\n",
    "    \n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    scores = []\n",
    "    for t in clean_tweets:\n",
    "        scores.append(sia.polarity_scores(t)['compound'])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(scores, bins=3, range=(-1,1), color='red')\n",
    "    ax.set_title(\"Most Common Sentiments in Tweets\")\n",
    "    ax.set_xticks([-0.7, 0, 0.7])\n",
    "    ax.set_xticklabels(['Negative', 'Neutral', 'Positive'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de26372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:39:16.076948Z",
     "start_time": "2021-06-28T15:39:15.731760Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_sentiments(clean_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-details",
   "metadata": {},
   "source": [
    "#### Popularité"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-australia",
   "metadata": {},
   "source": [
    "Par popularité, on entend \"à quel point les discussions à propos de notre sujet sont populaires\", en termes de nombre de retweets et de mises en favoris, et en termes de nombre followers et nombre de statuts des utilisateurs qui postent à ce sujet. Dans cette section, nous analyserons donc les colonnes `retweets`, `favorites`, `followers` et `statuses` de notre dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-battle",
   "metadata": {},
   "source": [
    "1. Calculer le nombre moyen de followers et de statuts des utilisateurs qui tweetent à propos de notre sujet (= la popularité de notre communauté). La médiane serait-elle ici plus pertinent que la moyenne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-hebrew",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:54:33.675152Z",
     "start_time": "2021-06-28T15:54:33.667914Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_followers = df['followers'].median()\n",
    "mean_statuses = df['statuses'].median()\n",
    "print(mean_followers)\n",
    "print(mean_statuses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-thanksgiving",
   "metadata": {},
   "source": [
    "2. Calculer la moyenne du nombre de retweets et de favoris de nos tweets (= la popularité des tweets sur notre sujet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-equation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:57:06.746900Z",
     "start_time": "2021-06-28T15:57:06.717100Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_retweets = df['retweets'].mean()\n",
    "mean_favorites = df['favorites'].mean()\n",
    "print(mean_retweets)\n",
    "print(mean_favorites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-poster",
   "metadata": {},
   "source": [
    "3. Quel est le tweet le plus populaire de notre requête ?\n",
    "\n",
    "_Indice : On peut considérer que le tweet le plus populaire est celui qui a le plus grand nombre cumulé de retweets et de favoris._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-apparatus",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:57:08.640826Z",
     "start_time": "2021-06-28T15:57:08.518361Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sum_favorites_retweets'] = df['favorites'] + df['retweets']\n",
    "top_tweet = df[df['sum_favorites_retweets'] == max(df['sum_favorites_retweets'])]\n",
    "top_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-norfolk",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:57:43.133168Z",
     "start_time": "2021-06-28T15:57:43.113831Z"
    }
   },
   "outputs": [],
   "source": [
    "top_tweet = top_tweet['full_text'].values[0]\n",
    "top_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-assembly",
   "metadata": {},
   "source": [
    "#### Localisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4ea49",
   "metadata": {},
   "source": [
    "Nous analyserons dans cette section l'origine des auteurs de nos tweets — lorsque cette localisation est connue (dépend des informations fournies par l'utilisateur dans son profil)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-watershed",
   "metadata": {},
   "source": [
    "1. Extraire la colonne `locations` de notre dataframe de tweets, et la stocker sous forme de liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-wedding",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:58:50.728120Z",
     "start_time": "2021-06-28T15:58:50.698862Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locations = df['location'].values\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-cooper",
   "metadata": {},
   "source": [
    "2. Utiliser la fonction `Nominatim()` du package `geopy.geocoders` pour effectuer un \"reverse geocoding\" (\"géocodage inverse\") des localisations.\n",
    "\n",
    "\n",
    "_Note : Le géocodage inverse consiste à obtenir la latitude et la longitude d'un lieu à partir de son nom. Par exemple, la fonction `Nominatim()` peut prendre comme argument \"New York, US\" et renvoyer la latitude et la longitude du centre de New York. En coulisses, cette fonction requête l'API de géocodage inverse \"Nominatim\" qui est gérée par OpenStreetMap, une version open-source de Google Maps._\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* L'objectif de cette question est de créer un **dictionnaire** contenant 2 clés : \"latitude\" et \"longitude\", et à l'intérieur de chaque clé une **liste** de toutes les latitudes (ou longitudes) de nos localisations.\n",
    "* Regardez l'exemple ci-dessous pour voir comment reverse géocoder une seule localisation. Vous pourrez ensuite faire de même sur toutes les localisations dans une boucle, en remplissant au fur et à mesure votre dictionnaire de résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-cooler",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T15:59:20.544222Z",
     "start_time": "2021-06-28T15:59:20.373038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisation\n",
    "geolocator = Nominatim(user_agent=\"my-app\")\n",
    "\n",
    "# Exemple\n",
    "location = 'New York, US'\n",
    "reverse_location = geolocator.geocode(location)\n",
    "lat = reverse_location.latitude\n",
    "lon = reverse_location.longitude\n",
    "print(lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-douglas",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:02:23.541535Z",
     "start_time": "2021-06-28T16:01:50.123395Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boucle sur toutes les localisations pour obtenir leurs coordonnées\n",
    "\n",
    "coordinates = {'latitude': [], 'longitude': []}\n",
    "\n",
    "for user_loc in tqdm(locations):\n",
    "    try:\n",
    "        location = geolocator.geocode(user_loc)\n",
    "        coordinates['latitude'].append(location.latitude)\n",
    "        coordinates['longitude'].append(location.longitude)         \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-transfer",
   "metadata": {},
   "source": [
    "3. Transformer le dictionnaire de la question précédente en dataframe (de 2 colonnes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-terrace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:02:23.640412Z",
     "start_time": "2021-06-28T16:02:23.546723Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_loc = pd.DataFrame(coordinates)\n",
    "df_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-pasta",
   "metadata": {},
   "source": [
    "4. Utiliser la fonction ci-dessous pour afficher les localisations sur une carte interactive.\n",
    "\n",
    "_Note : La fonction utilise le package `folium` pour créer la carte. Pas la peine de tout comprendre dans la fonction, il s'agit de détails de syntaxe liés à ce package ; en revanche il est intéressant de lire attentivement la \"docstring\", c'est-à-dire  la description de la function, afin de comprendre quels paramètres doivent être fournis._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-terminology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:04:57.556770Z",
     "start_time": "2021-06-28T16:04:57.536283Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_locations(df, lat, lon, kind='heatmap', save=False, zoom=2):\n",
    "    \"\"\"\n",
    "    Displays a map with 2 layers (OSM and ESRI satellite imagery) and a heatmap or markers\n",
    "    corresponding to the locations passed as a dataframe of latitudes and longitudes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df: dataframe\n",
    "        A dataframe containing at least 2 columns, corresponding to the latitude and \n",
    "        longitudes of the points you want to plot.\n",
    "        \n",
    "    lat: string\n",
    "        The name of the column containing the latitudes.\n",
    "    \n",
    "    lon: string\n",
    "        The name of the column containing the longitudes.\n",
    "    \n",
    "    kind: string, either 'heatmap' (default) or 'markers'\n",
    "        The kind of visualization you want to plot.\n",
    "    \n",
    "    save: boolean, optional, default: False\n",
    "        Save the map as 'map.html' in the current folder.\n",
    "        \n",
    "    zoom: int, between 0 and 20, optional, default: 2\n",
    "        The zoom of the base map. Lower is less zoomed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create map, with default OSM tile layer\n",
    "    m = folium.Map(location=[df[lat].mean(), df[lon].mean()], zoom_start=zoom)\n",
    "\n",
    "    # Create markers\n",
    "    locations = []\n",
    "    popups = []\n",
    "    for idx, row in df.iterrows():\n",
    "        locations.append([row[lat], row[lon]])\n",
    "        popups.append(([row[lat], row[lon]]))\n",
    "\n",
    "    # Add markers\n",
    "    if kind == 'markers':\n",
    "        s = folium.FeatureGroup(name='Points')\n",
    "        s.add_child(MarkerCluster(locations=locations, popups=popups))\n",
    "        m.add_child(s)\n",
    "\n",
    "    # Plot heatmap\n",
    "    elif kind == 'heatmap':\n",
    "        heatmap = df[[lat, lon]].values\n",
    "        m.add_child(HeatMap(heatmap, name='Heatmap', radius=12))\n",
    "    else:\n",
    "        print('Please specify a valid kind of map (\"markers\" or \"heatmap\").')\n",
    "    \n",
    "    # Add ability to see lat and lon onclick\n",
    "    m.add_child(folium.LatLngPopup())\n",
    "\n",
    "    # Add satellite layer\n",
    "    tile = folium.TileLayer(\n",
    "            tiles = 'https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "            attr = 'Esri',\n",
    "            name = 'Esri Satellite',\n",
    "            overlay = False,\n",
    "            control = True\n",
    "           ).add_to(m)\n",
    "\n",
    "    # Add layer control toggle\n",
    "    # We can control the display of both tile layers and overlay figures\n",
    "    folium.LayerControl().add_to(m)\n",
    "    \n",
    "    # Save map as HTML\n",
    "    if save:\n",
    "        m.save(\"map.html\")\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-innocent",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T16:04:59.150541Z",
     "start_time": "2021-06-28T16:04:58.306991Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_locations(df_loc, 'latitude', 'longitude', kind='markers', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-senate",
   "metadata": {},
   "source": [
    "#### Résutats finaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-cowboy",
   "metadata": {},
   "source": [
    "Comme résultat final de cette analyse, nous voulons une **liste** contenant les principaux résultats de notre analyse.\n",
    "\n",
    "Nous voulons que tout soit dans une liste unique, afin de pouvoir la **sauvegarder** puis utiliser l'API Google Sheets (section suivante) pour l'envoyer directement dans un **spreadsheet**, où elle constituera une **nouvelle ligne**.\n",
    "\n",
    "De cette façon, nous serons en mesure d'exécuter l'analyse à nouveau à intervalles réguliers, et d'ajouter simplement à chaque fois une nouvelle ligne de résultats à notre Google Sheets, afin de suivre l'évolution de notre sujet au fil du temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-publisher",
   "metadata": {},
   "source": [
    "1. Créer une liste finale contenant 13 éléments (dans cet ordre) :\n",
    "\n",
    "* la date de l'analyse (aujourd'hui), sous forme de string\n",
    "* le nombre de tweets que nous avons analysés\n",
    "* les 10 mots les plus fréquents sous forme de liste\n",
    "* les 10 emojis les plus fréquents sous forme de liste\n",
    "* les 10 hashtags les plus fréquents sous forme de liste\n",
    "* le sentiment moyen de nos tweets (score compound)\n",
    "* le nombre moyen de followers\n",
    "* le nombre moyen de statuts\n",
    "* le nombre moyen de retweets\n",
    "* le nombre moyen de favoris\n",
    "* le tweet le plus populaire\n",
    "* la liste de latitudes\n",
    "* la liste de longitudes\n",
    "\n",
    "_Indices : Nous avons déjà tout calculé ici, à l'exception de la date d'aujourd'hui ! Vous devez juste rassembler tous les résultats précédents (et éventuellement ajuster leur format). Pour la date d'aujourd'hui, vous pouvez utiliser `import datetime` puis `str(datetime.datetime.now())`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-bible",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T19:20:01.793242Z",
     "start_time": "2021-04-21T19:20:01.768029Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-laser",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T19:20:21.086335Z",
     "start_time": "2021-04-21T19:20:21.059411Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_insights = [str(datetime.datetime.now()),\n",
    "                  len(df),\n",
    "                  [t[0] for t in top10_words],\n",
    "                  [t[0] for t in top10_emojis],\n",
    "                  [t[0] for t in top10_hashtags],\n",
    "                  mean_sentiment,\n",
    "                  mean_followers,\n",
    "                  mean_statuses,\n",
    "                  mean_retweets,\n",
    "                  mean_favorites,\n",
    "                  top_tweet,\n",
    "                  coordinates['latitude'],\n",
    "                  coordinates['longitude'],\n",
    "                 ]\n",
    "final_insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-brooklyn",
   "metadata": {},
   "source": [
    "2. Convertir la liste finale en string et la sauvegarder sous forme de fichier texte nommé `analysis.txt`.\n",
    "\n",
    "_Conseils :_\n",
    "\n",
    "* Vous devrez utiliser la syntaxe `with open()`. Voici un exemple de cette syntaxe, à adapter à votre cas :\n",
    "\n",
    "```\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(content)\n",
    "```\n",
    "\n",
    "* Certains emojis peuvent provoquer des erreurs d'encoding lorsqu'ils sont sauvegardés dans un ficher texte. Si c'est le cas, vous pouvez utiliser la ligne suivante au moment de l'écriture de votre fichier, afin de convertir les emojis en caractères unicode : `f.write(str(content).encode('unicode-escape').decode(\"utf-8\"))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-village",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-21T23:59:23.358577Z",
     "start_time": "2021-04-21T23:59:23.323671Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('analysis.txt', 'w') as f:\n",
    "    f.write(str(final_insights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4ccda",
   "metadata": {},
   "source": [
    "## API Google Sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2bc79",
   "metadata": {},
   "source": [
    "Dans cette dernière section, vous serez guidés pour la dernière étape de ce projet Twitter : **envoyer les résultats de l'anayse dans un fichier Google Sheets**. L'objectif est de construire la brique finale de notre workflow, afin d'avoir un **code complet prêt à être automatisé ou réutilisé sur d'autres sujets**. \n",
    "\n",
    "Nous avons choisi Google Sheets car c'est un format facilement partageable, compris par tous les utilisateurs (même non-tech), et surtout parce que vous pouvez facilement l'utiliser pour **construire un dashboard de suivi** — par exemple avec **Data Studio**, un dashboard qui serait automatiquement mis à jour chaque semaine pour suivre visuellement la perception de votre sujet au fil du temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f762dd8",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd21211",
   "metadata": {},
   "source": [
    "Nous avons d'abord besoin de connecter ce notebook à votre **compte de service Google Cloud** (utilisez le PDF d'instructions disponible sur la plateforme créer un compte si besoin) grâce au package **`gspread`**.\n",
    "\n",
    "Pour vous authentifier, copiez-collez ci-dessous le **chemin de la clé** (fichier .json) qui a été téléchargée au moment de la création de votre compte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fe7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre clé devrait ressembler à ceci :\n",
    "path = \"/Users/Thomas/Downloads/databird-309423-c4ff87ee5aec.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentification avec gspread\n",
    "gc = gspread.service_account(filename=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa72d67",
   "metadata": {},
   "source": [
    "### Chargement des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398160c",
   "metadata": {},
   "source": [
    "1. Lire le fichier `analysis.txt` qui a été sauvegardé à la fin de l'analyse Twitter.\n",
    "\n",
    "_Indice : Vous pouvez ouvrir le fichier avec la fonction `open()`, puis le lire avec la méthode `.read()`. Ci-dessous, un exemple de syntaxe, à adapter à votre cas :_\n",
    "\n",
    "```\n",
    "f = open(filepath, 'r')\n",
    "f = f.read()\n",
    "f\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_path = '/Users/Thomas/Documents/Data Science X2/DataBird/Batch4-5/3. Python/J9 APIs 1/Cas Twitter/analysis.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ad769",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(analysis_path, \"r\")\n",
    "file = file.read()\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f3c8b",
   "metadata": {},
   "source": [
    "2. Quel est le type du fichier que vous venez de lire (type python) ? Convertissez-le en liste.\n",
    "\n",
    "_Indice : Vous aurez besoin de la fonction `ast.literal_eval()`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94391b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "row = ast.literal_eval(file)\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cb22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53551753",
   "metadata": {},
   "source": [
    "3. La liste contient différents types de données (string, int, dictionnaire, liste). Hors Google Sheets n'acceptera pas de mettre un dictionnaire ou une liste dans une cellule (c'est comme dans Excel : les seuls types de données que vous pouvez avoir dans une cellule sont du texte ou des chiffres, ou tout au plus des dates). Comment résoudriez-vous ce problème ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir tous les éléments non-numériques de la liste en strings\n",
    "formatted_row = [str(x) if not (type(x) == int or type(x) == float) else x for x in row]\n",
    "formatted_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb04a3",
   "metadata": {},
   "source": [
    "### Envoi au spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219ef29",
   "metadata": {},
   "source": [
    "Dans cette section, nous ouvrons une feuille de calcul Google Sheets avec `gspread` et nous y ajoutons une nouvelle ligne contenant la liste de résultats. Assurez-vous de bien **créer d'abord cette feuille de calcul** et **d'autoriser l'API Google Sheets**, en suivant les **instructions** disponibles sur la plateforme Databird (fichier PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66757672",
   "metadata": {},
   "source": [
    "1. Ouvrez votre spreadsheet avec `gspread` (aidez-vous du notebook de démo si besoin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ecc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet = gc.open_by_key('1c_Eij3Y-fAX7qaOQKSFOBivtV5Zb3oNprY3qE_PiNZU').sheet1\n",
    "worksheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb40188",
   "metadata": {},
   "source": [
    "2. Ajoutez les résultats de l'analyse Twitter (c'est-à-dire la liste) en tant que nouvelle ligne de votre feuille de calcul (à nouveau, aidez-vous de la démo pour utiliser la méthode la plus adéquate !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "worksheet.append_row(formatted_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a727a",
   "metadata": {},
   "source": [
    "[BONUS] 3. Ecrivez une longue fonction qui reprend toutes les étapes du projet (de la recherche initiale à l'envoi dans Google Sheets) pour pouvoir appliquer le workflow complet sur une nouvelle recherche de tweets (à passer en argument de la fonction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import emoji\n",
    "import gspread\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def twitter_analysis(query_params={'q':'tesla',\n",
    "                                   'lang':'en',\n",
    "                                   'since':\"2021-04-16\",\n",
    "                                   'until':\"2021-04-17\"}, \n",
    "                     full_output=False,\n",
    "                    ):\n",
    "    \n",
    "    print('Searching for tweets...')\n",
    "    \n",
    "    # Twitter API keys\n",
    "    API_KEY = 'diVvpJ68bD2T9Z7oSWcqRFSeh'\n",
    "    API_SECRET_KEY = 'A8WSmaeMahMkUDTI1oG8Q0OSaBCOswxCMCCK4JfkmyzsVBcE0R'\n",
    "    BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAJXNOgEAAAAAtUAkIFZgT38EsEGEMA%2BWXWZU3n4%3Daa9H6Jd9k7ddlaC92144L55mOqP9Nl4JBHhYLZyufTC50RNbxE'\n",
    "    \n",
    "    # Twitter authentication\n",
    "    auth = tweepy.AppAuthHandler(API_KEY, API_SECRET_KEY)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    # Twitter search\n",
    "    tweets = tweepy.Cursor(api.search,\n",
    "                           tweet_mode=\"extended\",\n",
    "                           **query_params,\n",
    "                          ).items(50)\n",
    "    tweets = list(tweets)\n",
    "    \n",
    "    print('Analyzing tweets...')\n",
    "    \n",
    "    # Information extraction\n",
    "    tweets_infos = []\n",
    "    for tweet in tqdm(tweets):\n",
    "        tweet_info = [tweet.full_text, \n",
    "                        tweet.created_at, \n",
    "                        [h['text'] for h in tweet.entities['hashtags']], \n",
    "                        tweet.user.location, \n",
    "                        tweet.user.followers_count, \n",
    "                        tweet.user.statuses_count, \n",
    "                        tweet.retweet_count, \n",
    "                        tweet.favorite_count] \n",
    "        tweets_infos.append(tweet_info)\n",
    "    \n",
    "    # Dataframe creation\n",
    "    df = pd.DataFrame(tweets_infos)\n",
    "    df.columns = ['full_text', 'created_at', 'hashtags', 'location',\n",
    "                  'followers', 'statuses', 'retweets', 'favorites']\n",
    "    df['clean_text'] = df['full_text'].apply(lambda x: re.sub(\"(http\\S+)|(@\\S+)\", \"\", x).replace('#', '').lower())\n",
    "    \n",
    "    # Tweet cleaning / corpus creation\n",
    "    tweets_text = [t[0] for t in tweets_infos]\n",
    "    corpus = []\n",
    "    for tweet in tweets_text:\n",
    "        clean = re.sub(\"(http\\S+)|(@\\S+)\", \"\", tweet).replace('#', '').lower()\n",
    "        clean = re.sub(r'[^\\w\\s\\U00010000-\\U0010ffff]', ' ', clean)\n",
    "        clean = clean.split()\n",
    "        collection_words = ['amazon']\n",
    "        clean = [word for word in clean if not word in collection_words]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        clean = [word for word in clean if not word in stop_words]\n",
    "        corpus.extend(clean)\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    top10_words = Counter(corpus).most_common(10)\n",
    "\n",
    "    # Sentiment analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    clean_tweets = df['clean_text'].values\n",
    "    sentiment_scores = [sia.polarity_scores(t)[\"compound\"] for t in clean_tweets]\n",
    "    mean_sentiment = np.mean(sentiment_scores)\n",
    "    \n",
    "    # Hashtags analysis\n",
    "    hashtags = list(df['hashtags'].values)\n",
    "    hashtags = list(itertools.chain(*hashtags))\n",
    "    hashtags = [h.lower() for h in hashtags]\n",
    "    top10_hashtags = Counter(hashtags).most_common(10)\n",
    "    \n",
    "    # Emojis analysis\n",
    "    emoji_corpus = []\n",
    "    for word in corpus:\n",
    "        for character in word:\n",
    "            if character in emoji.UNICODE_EMOJI['en']:\n",
    "                emoji_corpus.append(character)\n",
    "    top10_emojis = Counter(emoji_corpus).most_common(10)\n",
    "    \n",
    "    # Popularity analysis\n",
    "    mean_followers = df['followers'].mean()\n",
    "    mean_statuses = df['statuses'].mean()\n",
    "    mean_retweets = df['retweets'].mean()\n",
    "    mean_favorites = df['favorites'].mean()\n",
    "    df['sum_favorites_retweets'] = df['favorites'] + df['retweets']\n",
    "    top_tweet = df[df['sum_favorites_retweets'] == max(df['sum_favorites_retweets'])]\n",
    "    top_tweet = top_tweet['full_text'].values[0]\n",
    "    df = df.drop(columns=['sum_favorites_retweets'])\n",
    "    \n",
    "    # Location coordinates\n",
    "    locations = df.location.values\n",
    "    geolocator = Nominatim(user_agent=\"my-app\")\n",
    "    coordinates = {'latitude': [], 'longitude': []}\n",
    "    for user_loc in tqdm(locations):\n",
    "        try:\n",
    "            location = geolocator.geocode(user_loc)\n",
    "            if location:\n",
    "                coordinates['latitude'].append(location.latitude)\n",
    "                coordinates['longitude'].append(location.longitude)         \n",
    "        except:\n",
    "            pass\n",
    "    df_loc = pd.DataFrame(coordinates)\n",
    "    \n",
    "    # Final output\n",
    "    final_insights = [str(datetime.datetime.now()),\n",
    "                  len(df),\n",
    "                  [t[0] for t in top10_words],\n",
    "                  [t[0] for t in top10_emojis],\n",
    "                  [t[0] for t in top10_hashtags],\n",
    "                  mean_sentiment,\n",
    "                  mean_followers,\n",
    "                  mean_statuses,\n",
    "                  mean_retweets,\n",
    "                  mean_favorites,\n",
    "                  top_tweet,\n",
    "                  coordinates['latitude'],\n",
    "                  coordinates['longitude'],\n",
    "                 ]\n",
    "    \n",
    "    print('Sending to Google Sheets...')\n",
    "    \n",
    "    # Google Sheets authentication\n",
    "    path = \"/Users/Thomas/Downloads/databird-309423-c4ff87ee5aec.json\"\n",
    "    gc = gspread.service_account(filename=path)\n",
    "        \n",
    "    # Prepare final ouput for sending\n",
    "    formatted_row = [str(x) if not (type(x) == int or type(x) == float) else x for x in final_insights]\n",
    "    \n",
    "    # Send to Google sheets\n",
    "    worksheet = gc.open_by_key('1c_Eij3Y-fAX7qaOQKSFOBivtV5Zb3oNprY3qE_PiNZU').sheet1\n",
    "    worksheet.append_row(formatted_row)\n",
    "    \n",
    "    print('Done!')\n",
    "\n",
    "    if full_output:\n",
    "        return final_insights, df\n",
    "    else:\n",
    "        return final_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_insights, full_output = twitter_analysis(query_params={'q':'tesla',\n",
    "                                                             'lang':'en',\n",
    "                                                             'since':\"2021-04-16\",\n",
    "                                                             'until':\"2021-04-17\"},\n",
    "                                               full_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72722329",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5dcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64c49c",
   "metadata": {},
   "source": [
    "Félicitations, vous disposez maintenant d'un très bon outil d'analyse ! Vous pourriez placer le script complet dans un **fichier python .py** et l'exécuter périodiquement (par exemple chaque semaine), grâce à un **planificateur** de script comme **Airflow** (le plus connu). Vous auriez bientôt plusieurs lignes dans votre Google Sheets, que vous pourriez directement utiliser pour **construire et partager des dashboards d'analyse** (grâce à Data Studio) avec toute votre entreprise !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a298c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
